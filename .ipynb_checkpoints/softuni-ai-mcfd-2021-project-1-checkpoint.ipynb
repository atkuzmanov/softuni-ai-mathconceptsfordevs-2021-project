{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import math\n",
    "import cmath\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as p\n",
    "import matplotlib.pyplot as plt\n",
    "from turtle import *\n",
    "import re\n",
    "from sympy.ntheory import discrete_log\n",
    "from matplotlib.transforms import Affine2D\n",
    "import skimage.io\n",
    "import time\n",
    "# imports for egienvalues and eigenvectors\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic:\n",
    "\n",
    "Principal Component Analysis\n",
    "\n",
    "Sometimes a projection of a higher-dimensional to a lower-dimensional space is useful. It's extremely useful if we want to get some visual understanding of a, say, 15D space, in 3D or even 2D. One algorithm which allows us to project multidimensional data into fewer dimensions while keeping the most important shapes and structures is called principal component analysis (PCA). You can explore this using the following checklist:\n",
    "\n",
    "- [x] What are eigenvalues and eigenvectors?\n",
    "- [x] What is the eigenbasis? \n",
    "- [x] What is the spectrum of a matrix?\n",
    "- [x] How do we compute the eigenvalues and eigenvectors of a matrix?\n",
    "- [x] What is projection?\n",
    "- [x] How does projection conserve some shapes? Think about an object casting a shadow\n",
    "- [x] How is the projection problem related to eigenvalues and eigenvectors?\n",
    "- [x] What is PCA?\n",
    "- [x] What are principal components? How many components are there (as a function of dimensions of the original space)?\n",
    "- [ ] What is variance? What is explained variance?\n",
    "- [ ] How do principal components relate to explained variance?\n",
    "- [ ] How is PCA implemented? Implement and show.\n",
    "- [ ] Show some applications of PCA, e.g. reducing a 3D image to its first 2 principal components, plotting the 3D and 2D images.\n",
    "- [ ] Show a practical use of PCA, for example, trying to see features in a 15D space, projected in 3D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Motivation\n",
    "\n",
    "Here is the perspective: we are an experimenter. We are trying to understand some phenomenon by measuring various quantities (e.g. spectra, voltages, velocities, etc.) in our system. Unfortunately, we can not figure out what is happening because the data appears clouded, unclear and even redundant. This is not a trivial problem, but rather a fundamental obstacle in empirical science. Examples abound from complex systems such as neuroscience, photometry, meteorology and oceanography - the number of variables to measure can be unwieldy and at times even deceptive, because the underlying relationships can often be quite simple. Take for example a simple toy problem from physics diagrammed in Figure 1. Pretend we are studying the motion of the physicist’s ideal spring. This system consists of a ball of mass m attached to a massless, friction- less spring. The ball is released a small distance away from equilibrium (i.e. the spring is stretched). Because the spring is “ideal,” it oscillates indefinitely along the x-axis about its equilibrium at a set frequency. This is a standard problem in physics in which the motion along the x direction is solved by an explicit function of time. In other words, the underlying dynamics can be expressed as a function of a single variable x. However, being ignorant experimenters we do not know any of this. We do not know which, let alone how many, axes and dimensions are important to measure. Thus, we decide to measure the ball’s position in a three-dimensional space (since we live in a three dimensional world). Specifically, we place three movie cameras around our system of interest. At 200 Hz each movie camera records an image indicating a two dimensional position of the ball (a projection). Unfortunately, because of our ignorance, we do not even know what are the real “x”, “y” and “z” axes, so we choose three camera axes {~a, ~b,~c} at some arbitrary angles with respect to the system. The angles between our measurements might not even be 90o! Now, we record with the cameras for several minutes. The big question remains: how do we get from this data set to a simple equation of x? We know a-priori that if we were smart experimenters, we would have just measured the position along the xaxis with one camera. But this is not what happens in the real world. We often do not know which measurements best reflect the dynamics of our system in question. Furthermore, we sometimes record more dimensions than we actually need! Also, we have to deal with that pesky, real-world problem of noise. In the toy example this means that we need to deal with air, imperfect cameras or even friction in a less-than-ideal spring. Noise contaminates our data set only serving to obfuscate the dynamics further. This toy example is the challenge experimenters face everyday. [[2]](#2)[[3]](#3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But first...\n",
    "\n",
    "_Let's cover a few terms and definitions, so we have them at hand for clarity sake._\n",
    "\n",
    "_Please continue reading below and jump and check the [Definitions](#Definitions) section when needed for anything which is in the scope of this article._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are eigenvalues and eigenvectors?\n",
    "\n",
    "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. \n",
    "\n",
    "The corresponding eigenvalue, often denoted by λ is the factor by which the eigenvector is scaled.\n",
    "\n",
    "Let’s consider for an example that we want to build mathematical models (equations) where the input data is gathered from a large number of sources.\n",
    "\n",
    "It introduces its own sets of problems such as the large sparse matrix can end up taking a significant amount of space on a disk. Plus, it becomes extremely time-consuming for the model to train itself on the data. Furthermore, it is difficult to understand and visualize data with more than 3 dimensions, let alone a dataset of over 100+ dimensions. Hence, it would be ideal to somehow compress/transform this data into a smaller dataset.\n",
    "\n",
    "There is a solution. We can utilise Eigenvalues and Eigenvectors to reduce the dimension space. To elaborate, one of the key methodologies to improve efficiency in computationally intensive tasks is to reduce the dimensions after ensuring most of the key information is maintained.\n",
    "\n",
    "**Eigenvalues and Eigenvectors are the key tools to use in those scenarios.**\n",
    "\n",
    "**1. What Is An Eigenvector?**\n",
    "\n",
    "> Intuitively, an eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. [[Reference]](#TheMathematicsBehindPrincipalComponentAnalysis)\n",
    "\n",
    "For the sake of simplicity, let’s consider that we live in a two-dimensional world.\n",
    "\n",
    "- Alex’s house is located at coordinates [10,10] (x=10 and y =10). Let’s refer to it as vector A.\n",
    "\n",
    "- Furthermore, his friend Bob lives in a house with coordinates [20,20] (x=20 and y=20). I will refer to it as vector B.\n",
    "\n",
    "If Alex wants to meet Bob at his place then Alex would have to travel +10 points on the x-axis and +10 points on the y-axis. This movement and direction can be represented as a two-dimensional vector [10,10]. Let’s refer to it as vector C.\n",
    "\n",
    "We can see that vector A to B are related because vector B can be achieved by scaling (multiplying) the vector A by 2. This is because 2 x [10,10] = [20,20]. This is the address of Bob. Vector C also represents the movement for A to reach B.\n",
    "\n",
    "**The key to note is that a vector can contain the magnitude and direction of a movement.**\n",
    "\n",
    "We learned from the introduction above that large set of data can be represented as a matrix and we need to somehow compress the columns of the sparse matrix to speed up our calculations. Plus if we multiply a matrix by a vector then we achieve a new vector. The multiplication of a matrix by a vector is known as transformation matrices.\n",
    "\n",
    "**We can transform and change matrices into new vectors by multiplying a matrix with a vector. The multiplication of the matrix by a vector computes a new vector. This is the transformed vector.**\n",
    "    \n",
    "- Sometimes, the new transformed vector is just a scaled form of the original vector. This means that the new vector can be re-calculated by simply multiplying a scalar (number) to the original vector; just as in the example of vector A and B above.\n",
    "    \n",
    "- And other times, the transformed vector has no direct scalar relationship with the original vector which we used to multiply to the matrix.\n",
    "\n",
    "**If the new transformed vector is just a scaled form of the original vector then the original vector is known to be an eigenvector of the original matrix. Vectors that have this characteristic are special vectors and they are known as eigenvectors. Eigenvectors can be used to represent a large dimensional matrix.**\n",
    "\n",
    "Therefore, if our input is a large sparse matrix M then we can find a vector o that can replace the matrix M. The criteria is that the product of matrix M and vector o should be the product of vector o and a scalar n:\n",
    "\n",
    "$$M * o = n* o$$\n",
    "\n",
    "This means that a matrix M and a vector o can be replaced by a scalar n and a vector o.\n",
    "In this instance, o is the eigenvector and n is the eigenvalue and our target is to find o and n.\n",
    "\n",
    "Therefore an eigenvector is a vector that does not change when a transformation is applied to it, except that it becomes a scaled version of the original vector.\n",
    "\n",
    "Eigenvectors can help us calculating an approximation of a large matrix as a smaller vector. There are many other uses which I will explain later on in the article.\n",
    "\n",
    "Eigenvectors are used to make linear transformation understandable. Think of eigenvectors as stretching/compressing an X-Y line chart without changing their direction.\n",
    "\n",
    "**2. What is an Eigenvalue?**\n",
    "\n",
    "*Eigenvalue— The scalar that is used to transform (stretch) an Eigenvector.*\n",
    "\n",
    "**3. Where are Eigenvectors and Eigenvalues used?** \n",
    "\n",
    "There are multiple uses of eigenvalues and eigenvectors:\n",
    "\n",
    "1. Eigenvalues and Eigenvectors have their importance in linear differential equations where you want to find a rate of change or when you want to maintain relationships between two variables.\n",
    "\n",
    "*Think of eigenvalues and eigenvectors as providing summary of a large matrix*\n",
    "\n",
    "2. We can represent a large set of information in a matrix. Performing computations on a large matrix is a very slow process. To elaborate, one of the key methodologies to improve efficiency in computationally intensive tasks is to reduce the dimensions after ensuring most of the key information is maintained. Hence, one eigenvalue and eigenvector are used to capture key information that is stored in a large matrix. This technique can also be used to improve the performance of data churning components.\n",
    "\n",
    "3. Component analysis is one of the key strategies that is utilised to reduce dimension space without losing valuable information. The core of component analysis (PCA) is built on the concept of eigenvalues and eigenvectors. The concept revolves around computing eigenvectors and eigenvalues of the covariance matrix of the features.\n",
    "\n",
    "4. Additionally, eigenvectors and eigenvalues are used in facial recognition techniques such as EigenFaces.\n",
    "\n",
    "5. They are used to reduce dimension space. The technique of Eigenvectors and Eigenvalues are used to compress the data. As mentioned above, many algorithms such as PCA rely on eigenvalues and eigenvectors to reduce the dimensions.\n",
    "\n",
    "*Eigenvectors and eigenvalues are used to reduce noise in data. They can help us improve efficiency in computationally intensive tasks. They also eliminate features that have a strong correlation between them.*\n",
    "\n",
    "6. Occasionally we gather data that contains a large amount of noise. Finding important or meaningful patterns within the data can be extremely difficult. Eigenvectors and eigenvalues can be used to construct spectral clustering. They are also used in singular value decomposition.\n",
    "\n",
    "7. We can also use eigenvector to rank items in a dataset. They are heavily used in search engines and calculus.\n",
    "\n",
    "8. Lastly, in non-linear motion dynamics, eigenvalues and eigenvectors can be used to help us understand the data better as they can be used to transform and represent data into manageable sets.\n",
    "\n",
    "*It can be slow to compute eigenvectors and eigenvalues. The computation is $O(n^3)$*\n",
    "\n",
    "**4. What are the building blocks of Eigenvalues and Eigenvectors?**\n",
    "\n",
    "Let’s understand the foundations of Eigenvalues and Eigenvectors.\n",
    "\n",
    "Eigenvectors and eigenvalues revolve around the concept of matrices.\n",
    "\n",
    "Matrices are used in machine learning problems to represent a large set of information. Eigenvalues and eigenvectors are all about constructing one vector with one value to represent a large matrix. Sounds very useful, right?\n",
    "\n",
    "**What is a matrix?** \n",
    "\n",
    "- A matrix has a size of X rows and Y columns, like a table.\n",
    "\n",
    "- A square matrix is the one that has a size of n, implying that X and Y are equal.\n",
    "\n",
    "- A square matrix is represented as A. This is an example of a square matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & B & C \\\\\n",
    "D & E & F \\\\\n",
    "G & H & I\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Note: Matrix properties are out of the scope of this article.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we compute the eigenvalues and eigenvectors of a matrix?\n",
    "\n",
    "Calculating Eigenvectors and Eigenvalues\n",
    "\n",
    "*Although we don’t have to calculate the Eigenvalues and Eigenvectors by hand every time but it is important to understand the inner workings to be able to confidently use the algorithms.*\n",
    "\n",
    "- The eigenvector is an array with n entries where n is the number of rows (or columns) of a square matrix. The eigenvector is represented as x.\n",
    "\n",
    "- Key Note: The direction of an eigenvector does not change when a linear transformation is applied to it.\n",
    "\n",
    "- Therefore, Eigenvector should be a non-null vector\n",
    "\n",
    "- We are required to find a number of values, known as eigenvalues such that\n",
    "\n",
    "$$ A * x = \\lambda * x$$\n",
    "\n",
    "The above equation states that we need to find eigenvalue (lambda) and eigenvector (x) such that when we multiply a scalar lambda (eigenvalue) to the vector x (eigenvector) then it should equal to the linear transformation of the matrix A once it is scaled by vector x (eigenvector).\n",
    "\n",
    "**Eigenvalues are represented as lambda.**\n",
    "\n",
    "**Note: The above equation should not be invertible.**\n",
    "\n",
    "There are two special keywords which we need to understand: Determinant of a matrix and an identity matrix.\n",
    "\n",
    "The **determinant of a matrix** is a number that is computed from a square matrix. In a nutshell, the diagonal elements are multiplied by each other and then they are subtracted together. We need to ensure that the determinant of the matrix is 0.\n",
    "\n",
    "We need an **Identity Matrix**. An identity square matrix is a matrix that has 1 in diagonal and all of its elements are 0. The identity matrix is represented as I:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We can represent\n",
    "\n",
    "$$A * x = \\lambda * x$$\n",
    "\n",
    "As:\n",
    "\n",
    "$$A * x - \\lambda * x = 0$$\n",
    "\n",
    "Now, we need to compute a characteristic equation.\n",
    "\n",
    "$$|A - \\lambda * I| = 0$$\n",
    "\n",
    "\n",
    "Subsequently:\n",
    "\n",
    "$$Determinant(A - \\lambda * I) = 0$$\n",
    "\n",
    "### How do I calculate Eigenvalue?\n",
    "\n",
    "The task is to find Eigenvalues of size n for a matrix A of size n.\n",
    "\n",
    "Therefore, the aim is to find Eigenvector and Eigenvalues of A such that:\n",
    "\n",
    "$$ A * Eigenvector — Eigenvalue * EigenVector = 0 $$\n",
    "\n",
    "**Find Lambda Such that:**\n",
    "\n",
    "$$ Determinant(A — \\lambda * I) = 0$$\n",
    "\n",
    "Based on the concepts from above:\n",
    "\n",
    "$\\lambda * I$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 0 & 0 \\\\\n",
    "0 & \\lambda & 0 \\\\\n",
    "0 & 0 & \\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "if A is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & B & C \\\\\n",
    "D & E & F \\\\\n",
    "G & H & I\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then $A - \\lambda * I$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & B & C \\\\\n",
    "D & E & F \\\\\n",
    "G & H & I\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 0 & 0 \\\\\n",
    "0 & \\lambda & 0 \\\\\n",
    "0 & 0 & \\lambda\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "A-\\lambda & B & C \\\\\n",
    "D & E-\\lambda & F \\\\\n",
    "G & H & I-\\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally calculate the determinant of $(A-\\lambda*I)$ as:\n",
    "\n",
    "$$\n",
    "A-\\lambda\n",
    "\\begin{bmatrix}\n",
    "E-\\lambda & F \\\\\n",
    "H & I-\\lambda\n",
    "\\end{bmatrix}\n",
    "-B\n",
    "\\begin{bmatrix}\n",
    "D & F \\\\\n",
    "G & I-\\lambda\n",
    "\\end{bmatrix}\n",
    "+C\n",
    "\\begin{bmatrix}\n",
    "D & E-\\lambda \\\\\n",
    "G & H\n",
    "\\end{bmatrix}\n",
    "=0\n",
    "$$\n",
    "\n",
    "Once the equation above is solved, we will get the values of lambda $\\lambda$. \n",
    "*These values are the Eigenvalues.*\n",
    "\n",
    "Once we have calculated eigenvalues, we can calculate the Eigenvectors of matrix A by using Gaussian Elimination.\n",
    "\n",
    "Gaussian elimination is about converting the matrix to row echelon form. Finally, it is about solving the linear system by back substitution.\n",
    "\n",
    "An explanation of Gaussian elimination is out of the scope of this article so that we can concentrate on Eigenvalues and Eigenvectors.\n",
    "\n",
    "**Once we have the Eigenvalues, we can find Eigenvector for each of the Eigenvalues. We can substitute the eigenvalue in the lambda and we will achieve an eigenvector.**\n",
    "\n",
    "$$(A - \\lambda * I) * x = 0$$\n",
    "\n",
    "*Therefore if a square matrix has a size n then we will get n eigenvalues and as a result, n eigenvectors will be computed to represent the matrix.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A practical example: Calculating Eigenvalue and Eigenvector\n",
    "\n",
    "**Let’s find eigenvalue of the following matrix A:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "First, multiply lambda to an identity matrix and then subtract the two matrices:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "-\\lambda\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We need to compute a determinant of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2-\\lambda & -1 \\\\\n",
    "4 & 3-\\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "det\n",
    "\\begin{bmatrix}\n",
    "2-\\lambda & -1 \\\\\n",
    "4 & 3-\\lambda\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\lambda^2 - 5\\lambda + 10\n",
    "$$\n",
    "\n",
    "Once we solve the quadratic equation above, we will yield two Eigenvalues:\n",
    "\n",
    "$$\n",
    "\\frac{5}{2} + i\\frac{\\sqrt{15}}{2},\n",
    "\\frac{5}{2} - i\\frac{\\sqrt{15}}{2}\n",
    "$$\n",
    "\n",
    "**Now that we have computed Eigenvalues, let’s calculate Eigenvectors:**\n",
    "\n",
    "Take the first Eigenvalue (Lambda $\\lambda$) and substitute the eigenvalue into the following equation:\n",
    "\n",
    "$$(A - \\lambda * I)$$\n",
    "\n",
    "We plug in the first Eigenvalue (Lambda $\\lambda$) to get the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\left(\\frac{5}{2} + i\\frac{\\sqrt{15}}{2}\\right)\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-\\frac{1}{2}-i\\frac{\\sqrt{15}}{2} & -1 \\\\\n",
    "4 & \\frac{1}{2}-i\\frac{\\sqrt{15}}{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This gives us the following Eigenvector, which is for the first eigenvalue:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 + \\sqrt{15}i \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This Eigenvector now represents the key information of matrix A.\n",
    "\n",
    "If we do the same for the other Eigenvalue we will get:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 - \\sqrt{15}i \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Eigenvalues and Eigenvectors in Python\n",
    "\n",
    "Although we don’t have to calculate the Eigenvalues and Eigenvectors by hand but it is important to understand the inner workings to be able to confidently use the algorithms. Furthermore, It is very straightforward to calculate eigenvalues and eigenvectors in Python.\n",
    "\n",
    "We can use `numpy.linalg.eig` module. It takes in a square matrix as the input and returns eigenvalues and eigenvectors. It also raises an `LinAlgError` if the eigenvalue computation does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[2.5+1.93649167j 2.5-1.93649167j]\n",
      "\n",
      "\n",
      "Eigenvectors:\n",
      "[[-0.1118034 +0.4330127j -0.1118034 -0.4330127j]\n",
      " [ 0.89442719+0.j         0.89442719-0.j       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "input = np.array([[2,-1],[4,3]])\n",
    "w, v = LA.eig(input)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(w)\n",
    "print(\"\\n\")\n",
    "print(\"Eigenvectors:\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the eigenbasis? \n",
    "\n",
    "Eigenbasis is a basis for a vector space consisting entirely of eigenvectors.\n",
    "\n",
    "### Eigenbasis and Diagonalization\n",
    "\n",
    "Given that we know that a transformation can have up to $n$ Eigenvectors, where $n$ is the number of rows, what happens if we use the Eigenvectors as a **change of basis**, by multiplying the transformation by the matrix of the Eigenvectors?\n",
    "\n",
    "As it turns out, converting the transformation to an Eigenbasis, if possible, (a conversion otherwise known as **Eigendecomposition**) is an incredibly useful conversion because of what happens to the transformation when it is converted in such a way.\n",
    "\n",
    "Take for example, the matrix \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "\n",
    "This matrix scales by a factor of 2 along the y-axis, shears along the xz-axis by a factor of 1.\n",
    "\n",
    "This transformation has Eigenvalues $\\lambda = 2$ and $\\lambda = 1$ with algebraic multiplicity 2.\n",
    "\n",
    "It also has Eigenvectors \n",
    "$\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}$\n",
    ", for \n",
    "$\\lambda = 2$, and $\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-1 \\\\\n",
    "1\n",
    "\\end{bmatrix}$\n",
    "and\n",
    "$\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}$\n",
    ", for $\\lambda = 1$.\n",
    "\n",
    "These Eigenvectors can be arranged into a new matrix called an **Eigenbasis**:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the inverse of the Eigenbasis can be found too:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0& 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's consider what happens if we change the basis of our matrix by premultiplying by the inverse of the Eigenbasis, then postmultiplying by the Eigenbasis (a transformation also known as an **Eigendecomposition**):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0& 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here it is important to note the following -  the result of changing the basis to a matrix to its Eigenbasis is that the matrix is put into a **Diagonalized form**. This is extremely useful, because while the matrix is in a diagonalized form, we can represent it like this:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\\cdot I\n",
    "$$\n",
    "\n",
    "Thus, if we want to apply any matrix multiplication operation to the matrix in its diagonalized form, it is the same as applying a matrix-vector optimization. Computer Scientists will recognize this as a huge performance win, since an \n",
    "$O(N^2)$ operation just became $O(N)$. Say for example we wanted to calcalculate the 16th power of the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Conventionally, this would take $9^2 \\times 16 = 1296$ operations. If we did the same thing on the diagonal, we can exploit the fact that we are exponentiating by powers of two and same thing would take just three barrel-shift operations, preceded by and followed by a normal matrix multiplication to undo the diagonalization.\n",
    "\n",
    "Given a set of eigenvectors and eigenvalues for a matrix, we can re-construct the original matrix. Why is this the case? Notice that when we decomposed the matrix, we did the following:\n",
    "\n",
    "$$\n",
    "AP = PD \\\\\n",
    "P^{-1}AP = D\n",
    "$$\n",
    "\n",
    "Where $P$ was our matrix of eigenvectors, $A$ was our original matrix that underwent eigendecomposition and $D$ is the eigendecomposed matrix.\n",
    "\n",
    "A property of eigenvalues is that multiplying the original matrix $A$ by an eigenvector $V$ is the same as multiplying that eigenvector by its eigenvalue $\\lambda$. All the multiplication does in both cases is scale the vector.\n",
    "\n",
    "This is the same thing if you multiply by a matrix that only has elements on the diagonal - the effect is scaling, regardless of whether the multiplication was a premultiplication or a postmultiplication. So it stands to reason that if you were to arrange the eigenvalues into a diagonal matrix $E$ with their columns corresponding to each eigenvector in the matrix $P$, then the following, just like above with $AP = PD$, holds true:\n",
    "\n",
    "$$AP = PE$$\n",
    "\n",
    "Lets see if this checks out:\n",
    "\n",
    "$$\n",
    "AP = \n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 2 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE = \n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 2 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Same thing! So now we can do the same thing as before - postmultiply both sides by $P^{-1}$ and it should be the case that we recover $A$, eg:\n",
    "\n",
    "$$\n",
    "APP^{-1} = PEP^{-1} \\\\\n",
    "A = PEP^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0& 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the spectrum of a matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now that we have covered some ground, before we proceed, let's step back and cover some Fundamental theory of matrix eigenvectors and eigenvalues._\n",
    "\n",
    "### Fundamental theory of matrix eigenvectors and eigenvalues \n",
    "[[1]](#1)\n",
    "\n",
    "A (non-zero) vector $v$ of dimension $N$ is an eigenvector of a square $N \\times N$ matrix $A$ if it satisfies the linear equation:\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "where $\\lambda$ is a scalar, termed the **eigenvalue** corresponding to $v$. That is, the eigenvectors are the vectors that the linear transformation $A$ merely elongates or shrinks, and the amount that they elongate/shrink by is the **eigenvalue**. The above equation is called the **eigenvalue equation** or the **eigenvalue problem**.\n",
    "\n",
    "This yields an equation for the eigenvalues:\n",
    "\n",
    "$$ p\\left(\\lambda \\right)=\\det \\left(\\mathbf{A} -\\lambda \\mathbf{I} \\right)=0$$\n",
    "\n",
    "We call $p(\\lambda)$ the **characteristic polynomial**, and the equation, called the **characteristic equation**, is an $Nth$ order polynomial equation in the unknown $\\lambda$. This equation will have $N\\lambda$ distinct solutions, where $1 \\leq N\\lambda \\leq N$. The set of solutions, that is, the eigenvalues, is called the **spectrum** of $A$.\n",
    "\n",
    "We can factor $p$ as:\n",
    "\n",
    "$$p\\left(\\lambda \\right)=\\left(\\lambda -\\lambda_{1}\\right)^{n_{1}}\\left(\\lambda -\\lambda _{2}\\right)^{n_{2}}\\cdots \\left(\\lambda -\\lambda _{N_{\\lambda }}\\right)^{n_{N_{\\lambda }}}=0$$\n",
    "\n",
    "The integer $n_i$ is termed the algebraic multiplicity of eigenvalue $\\lambda_i$. If the field of scalars is algebraically closed, the algebraic multiplicities sum to $N$:\n",
    "\n",
    "$${\\sum \\limits _{i=1}^{N_{\\lambda }}{n_{i}}=N}$$\n",
    "\n",
    "For each eigenvalue $\\lambda_i$, we have a specific eigenvalue equation:\n",
    "\n",
    "$${\\displaystyle \\left(\\mathbf {A} -\\lambda _{i}\\mathbf {I} \\right)\\mathbf {v} =0}$$\n",
    "\n",
    "There will be $1 \\leq m_i \\leq n_i$ linearly independent solutions to each eigenvalue equation. The linear combinations of the $m_i$ solutions are the eigenvectors associated with the eigenvalue $\\lambda_i$. The integer mi is termed the geometric multiplicity of $$\\lambda_i$$. It is important to keep in mind that the algebraic multiplicity $n_i$ and geometric multiplicity $m_i$ may or may not be equal, but we always have $m_i \\leq n_i$. The simplest case is of course when $m_i = n_i = 1$. The total number of linearly independent eigenvectors, $N_v$, can be calculated by summing the geometric multiplicities:\n",
    "\n",
    "$${\\displaystyle \\sum \\limits _{i=1}^{N_{\\lambda }}{m_{i}}=N_{\\mathbf {v} }}$$\n",
    "\n",
    "The eigenvectors can be indexed by eigenvalues, using a double index, with $v_{ij}$ being the $jth$ eigenvector for the $ith$ eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index $v_k$, with $k = 1, 2, ..., N_v$.\n",
    "\n",
    "_Note: algebraic multiplicity and geometric multiplicity are out of the scope of this article._\n",
    "\n",
    "### Spectrum of a matrix \n",
    "[[Spectrum of a matrix]](#Spectrumofamatrix)\n",
    "\n",
    "In mathematics, the spectrum of a matrix is the set of its eigenvalues. More generally, if ${\\displaystyle T\\colon V\\to V}$ is a linear operator over any finite-dimensional vector space, its spectrum is the set of scalars ${\\displaystyle \\lambda }$  such that ${\\displaystyle T-\\lambda I}$ I is not **invertible**. The determinant of the matrix equals the product of its eigenvalues. Similarly, the **trace** of the matrix equals the sum of its eigenvalues. From this point of view, we can define the **pseudo-determinant** for a **singular matrix** to be the product of its nonzero eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is projection?\n",
    "\n",
    "### Projection (linear algebra)\n",
    "[[Projection (linear algebra)]](#Projectionlinearalgebra)\n",
    "\n",
    "In linear algebra and functional analysis, a **projection** is a linear transformation ${\\displaystyle P}$ from a vector space to itself such that ${\\displaystyle P^{2}=P}$. That is, whenever ${\\displaystyle P}$ is applied twice to any value, it gives the same result as if it were applied once (idempotent). It leaves its image unchanged. Though abstract, this definition of \"projection\" formalizes and generalizes the idea of graphical projection. One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object.\n",
    "\n",
    "<img src=\"./resources/252px-Orthogonal_projection.svg.png\" width=\"252\" height=\"252\"/>\n",
    "\n",
    "<div style=\"text-align: center\">The transformation $P$ is the orthogonal projection onto the line $m$.</div>\n",
    "\n",
    "#### Projection (linear algebra) definitions\n",
    "\n",
    "A projection on a vector space ${\\displaystyle V}$ is a linear operator ${\\displaystyle P:V\\to V}$ such that ${\\displaystyle P^{2}=P}$.\n",
    "\n",
    "When ${\\displaystyle V}$ has an inner product and is complete (i.e. when ${\\displaystyle V}$ is a Hilbert space) the concept of orthogonality can be used. A projection ${\\displaystyle P}$ on a Hilbert space ${\\displaystyle V}$ is called an **orthogonal projection** if it satisfies ${\\displaystyle \\langle P\\mathbf {x} ,\\mathbf {y} \\rangle =\\langle \\mathbf {x} ,P\\mathbf {y} \\rangle }$ for all ${\\displaystyle \\mathbf {x} ,\\mathbf {y} \\in V}$. A projection on a Hilbert space that is not orthogonal is called an **oblique projection**.\n",
    "\n",
    "_**Projection matrix**_\n",
    "\n",
    "- In the finite-dimensional case, a square matrix ${\\displaystyle P}$ is called a **projection matrix** if it is equal to its square, i.e., if ${\\displaystyle P^{2}=P}$.\n",
    "\n",
    "- A square matrix ${\\displaystyle P}$ is called an **orthogonal projection matrix** if ${\\displaystyle P^{2}=P=P^{\\mathrm {T}}}$ for a real matrix, and respectively ${\\displaystyle P^{2}=P=P^{*}}$ for a complex matrix, where ${\\displaystyle P^{\\mathrm {T}}}$ denotes the transpose of ${\\displaystyle P}$ and ${\\displaystyle P^{*}}$ denotes the adjoint of ${\\displaystyle P}$.\n",
    "\n",
    "- A projection matrix that is not an orthogonal projection matrix is called an **oblique projection matrix**.\n",
    "\n",
    "The eigenvalues of a projection matrix must be 0 or 1.\n",
    "\n",
    "#### Projection (linear algebra) definitions\n",
    "\n",
    "___Orthogonal projection___\n",
    "\n",
    "For example, the function which maps the point ${\\displaystyle (x,y,z)}$ in three-dimensional space ${\\displaystyle \\mathbb{R} ^{3}}$ to the point ${\\displaystyle (x,y,0)}$ is an orthogonal projection onto the $xy$-plane. This function is represented by the matrix:\n",
    "\n",
    "${\\displaystyle P={\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}}.}$\n",
    "\n",
    "The action of this matrix on an arbitrary vector is:\n",
    "\n",
    "${\\displaystyle P{\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}}={\\begin{bmatrix}x\\\\y\\\\0\\end{bmatrix}}}$\n",
    "\n",
    "To see that ${\\displaystyle P}$ is indeed a projection, i.e., ${\\displaystyle P=P^{2}}$, we compute:\n",
    "\n",
    "$${\\displaystyle P^{2}{\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}}=P{\\begin{bmatrix}x\\\\y\\\\0\\end{bmatrix}}={\\begin{bmatrix}x\\\\y\\\\0\\end{bmatrix}}=P{\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}}}$$\n",
    "\n",
    "Observing that ${\\displaystyle P^{\\mathrm {T} }=P}$ shows that the projection is an orthogonal projection.\n",
    "\n",
    "___Oblique projection___\n",
    "\n",
    "A simple example of a non-orthogonal (oblique) projection (for definition see below) is:\n",
    "\n",
    "${\\displaystyle P={\\begin{bmatrix}0&0\\\\\\alpha &1\\end{bmatrix}}}$\n",
    "\n",
    "Via matrix multiplication, one sees that:\n",
    "\n",
    "$${\\displaystyle P^{2}={\\begin{bmatrix}0&0\\\\\\alpha &1\\end{bmatrix}}{\\begin{bmatrix}0&0\\\\\\alpha &1\\end{bmatrix}}={\\begin{bmatrix}0&0\\\\\\alpha &1\\end{bmatrix}}=P}$$\n",
    "\n",
    "proving that ${\\displaystyle P}$ is indeed a projection.\n",
    "\n",
    "The projection ${\\displaystyle P}$ is orthogonal if and only if ${\\displaystyle \\alpha =0}$ because only then ${\\displaystyle P^{\\mathrm {T} }=P}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector projection\n",
    "\n",
    "[[Vector projection]](#Vectorprojection)\n",
    "\n",
    "The vector projection of a vector $a$ on (or onto) a nonzero vector $b$, sometimes denoted ${\\displaystyle \\operatorname {proj} _{\\mathbf {b} }\\mathbf {a} }$ (also known as the **vector component** or **vector resolution** of $a$ in the direction of $b$), is the orthogonal projection of $a$ onto a straight line parallel to $b$. It is a vector parallel to $b$, defined as:\n",
    "\n",
    "$${\\displaystyle \\mathbf {a} _{1}=a_{1}\\mathbf {\\hat {b}}}$$\n",
    "\n",
    "where ${\\displaystyle a_{1}}$ is a scalar, called the scalar projection of $a$ onto $b$, and ${\\hat{b}}$ is the unit vector in the direction of $b$.\n",
    "\n",
    "In turn, the scalar projection is defined as:\n",
    "\n",
    "$${\\displaystyle a_{1}=\\left\\|\\mathbf {a} \\right\\|\\cos \\theta =\\mathbf {a} \\cdot \\mathbf {\\hat {b}} =\\mathbf {a} \\cdot {\\frac {\\mathbf {b} }{\\left\\|\\mathbf {b} \\right\\|}}}$$\n",
    "\n",
    "where the operator $⋅$ denotes a dot product, $‖a‖$ is the length of $a$, and $θ$ is the angle between $a$ and $b$.\n",
    "\n",
    "<img src=\"./resources/200px-Projection_and_rejection.png\" width=\"200\" />\n",
    "\n",
    "<div style=\"text-align: center\">Projection of $a$ on $b$ ($a_1$), and rejection of $a$ from $b$ ($a_2$).</div>\n",
    "\n",
    "The scalar projection is equal to the length of the vector projection, with a minus sign if the direction of the projection is opposite to the direction of $b$. The vector component or vector resolute of $a$ perpendicular to $b$, sometimes also called the **vector rejection** of $a$ from $b$ (denoted ${\\displaystyle \\operatorname {oproj} _{\\mathbf {b} }\\mathbf {a}}$ is the orthogonal projection of $a$ onto the plane orthogonal to $b$. Both the projection $a_1$ and rejection $a_2$ of a vector $a$ are vectors, and their sum is equal to $a$, which implies that the rejection is given by: ${\\displaystyle \\mathbf {a} _{2}=\\mathbf {a} -\\mathbf {a} _{1}.}$\n",
    "\n",
    "<img src=\"./resources/248px-Projection_and_rejection_2.png\" width=\"248\"/>\n",
    "\n",
    "<div style=\"text-align: center\">When $90° < θ ≤ 180°$, $a_1$ has an opposite direction with respect to $b$.</div>\n",
    "\n",
    "#### Notation\n",
    "\n",
    "Typically, a vector projection is denoted in a bold font (e.g. $\\pmb{a_{1}}$), and the corresponding scalar projection with normal font (e.g. $a_1$). In some cases, especially in handwriting, the vector projection is also denoted using a diacritic above or below the letter (e.g., ${\\displaystyle {\\vec {a}}_{1}}$ or $\\underline{a}_1$). The vector projection of $a$ on $b$ and the corresponding rejection are sometimes denoted by $a_{∥b}$ and $a_{⊥b}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does projection conserve some shapes? Think about an object casting a shadow.\n",
    "\n",
    "\n",
    "### Projection (mathematics)\n",
    "[[Projection (mathematics)]](#Projectionmathematics)\n",
    "\n",
    "In mathematics, a **projection** is a mapping of a set (or other mathematical structure) into a subset (or sub-structure), which is equal to its square for mapping composition (or, in other words, which is idempotent). The restriction to a subspace of a projection is also called a projection, even if the idempotence property is lost. An everyday example of a projection is the casting of shadows onto a plane (paper sheet). The projection of a point is its shadow on the paper sheet. The shadow of a point on the paper sheet is this point itself (idempotency). The shadow of a three-dimensional sphere is a closed disk. Originally, the notion of projection was introduced in Euclidean geometry to denote the projection of the Euclidean space of three dimensions onto a plane in it, like the shadow example. The two main projections of this kind are:\n",
    "\n",
    "- The projection from a point onto a plane or central projection: If $C$ is a point, called the center of projection, then the projection of a point $P$ different from $C$ onto a plane that does not contain $C$ is the intersection of the line $CP$ with the plane. The points $P$ such that the line $CP$ is parallel to the plane does not have any image by the projection, but one often says that they project to a point at infinity of the plane (see projective geometry for a formalization of this terminology). The projection of the point $C$ itself is not defined.\n",
    "\n",
    "- The projection parallel to a direction $D$, onto a plane or parallel projection: The image of a point $P$ is the intersection with the plane of the line parallel to $D$ passing through $P$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D projection \n",
    "\n",
    "[[3Dprojection]](#3Dprojection)\n",
    "\n",
    "A **3D projection** (or **graphical projection**) is a design technique used to display a **three-dimensional (3D)** object on a **two-dimensional (2D)** surface. These projections rely on visual perspective and aspect analysis to project a complex object for viewing capability on a simpler plane.\n",
    "\n",
    "3D projections use the primary qualities of an object's basic shape to create a map of points, that are then connected to one another to create a visual element. The result is a graphic that contains conceptual properties to interpret that the figure or image is not actually flat (2D), but rather, is a solid object (3D) being viewed on a 2D display.\n",
    "\n",
    "3D objects are largely displayed on two-dimensional mediums (i.e. paper and computer monitors). As such, graphical projections are a commonly used design element; notably, in engineering drawing, drafting, and computer graphics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel projection \n",
    "[[Parallel projection]](#Parallelprojection)\n",
    "\n",
    "A **parallel projection** is a projection of an object in three-dimensional space onto a fixed plane, known as the projection plane or image plane, where the rays, known as lines of sight or projection lines, are parallel to each other. It is a basic tool in descriptive geometry. The projection is called orthographic if the rays are perpendicular (orthogonal) to the image plane, and oblique or skew if they are not.\n",
    "\n",
    "\n",
    "<img src=\"./resources/Cube-parallel-proj-s.svg.png\" width=\"300\"/>\n",
    "\n",
    "<div style=\"text-align: center\">Two parallel projections of a cube. In an orthographic projection (at left), the projection lines are perpendicular to the image plane (pink). In an oblique projection (at right), the projection lines are at a skew angle to the image plane.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the projection problem related to eigenvalues and eigenvectors?\n",
    "\n",
    "From the [Projection (linear-algebra)](#Projection-(linear-algebra)) and more specifically from the **Projection matrix** section we know that the eigenvalues of a projection matrix must be **0** or **1**.\n",
    "\n",
    "The included images illustrate the orthogonal projection(the general case), the orthogonal projection of a vector $𝑣∈𝐸$ ($𝑣$ is an eigenvector corresponding to the eigenvalue $1$), respectively the orthogonal projection of a vector in $𝐸⊥$ ($𝑣$ is an eigenvector corresponding to $0$).\n",
    "[[Reference]](#eigenandprojection2)\n",
    "\n",
    "\n",
    "<img src=\"./resources/orthogonal-projection-1/VVgp3.png\" width=\"196\"/>\n",
    "<img src=\"./resources/orthogonal-projection-1/maKtj.png\" width=\"196\"/>\n",
    "<img src=\"./resources/orthogonal-projection-1/TObdO.png\" width=\"196\"/>\n",
    "\n",
    "However, not all eigenvectors are in the projection plane of a (projection) matrix, unless it is the identity matrix.\n",
    "If $P$ is a projection matrix, then it has two eigenvalues:  $0$  and  $1$  (we will assume  $𝑃≠0$  and  $𝑃≠𝐼$  for simplicity). Every nonzero vector in the column space of $P$ (that is, in the projection plane) is an eigenvector corresponding to the eigenvalue $1$. Every nonzero vector orthogonal to the column space of $P$ will be an eigenvector corresponding to the eigenvalue $0$.[[Reference]](#eigenandprojection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables while retaining as much as possible of the variation present in the data set. This is achieved by transforming to a new set of variables, the principal components (PCs), which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.\n",
    "\n",
    "For example lets say that we have 15 dimensions but we can only view a 3 dimensional (3D) space. PCA solves this for us by reducing the dimensions from 15 to 3. Another example is reducing a 3D image to its first 2 principal components and plotting the 3D as a 2D image.\n",
    "\n",
    "So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible. [[Reference]](#ASTEP-BY-STEPEXPLANATIONOFPRINCIPALCOMPONENTANALYSIS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Motivation\n",
    "\n",
    "Let’s say that you want to predict what the gross domestic product (GDP) of the United States will be for 2017. You have lots of information available: the U.S. GDP for the first quarter of 2017, the U.S. GDP for the entirety of 2016, 2015, and so on. You have any publicly-available economic indicator, like the unemployment rate, inflation rate, and so on. You have U.S. Census data from 2010 estimating how many Americans work in each industry and American Community Survey data updating those estimates in between each census. You know how many members of the House and Senate belong to each political party. You could gather stock price data, the number of IPOs occurring in a year, and how many CEOs seem to be mounting a bid for public office. Despite being an overwhelming number of variables to consider, this just scratches the surface.\n",
    "\n",
    "Basically you have a lot of variables to consider.\n",
    "\n",
    "If you’ve worked with a lot of variables before, you know this can present problems. Do you understand the relationships between each variable?\n",
    "\n",
    "You might ask the question, \"How do I take all of the variables I’ve collected and focus on only a few of them?\" In technical terms, you want to \"reduce the dimension of your feature space.\"\n",
    "\n",
    "Somewhat unsurprisingly, reducing the dimension of the feature space is called \"**dimensionality reduction**\". There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "\n",
    "- Feature Elimination\n",
    "- Feature Extraction\n",
    "\n",
    "**Feature elimination** is what it sounds like: we reduce the feature space by eliminating features. In the GDP example above, instead of considering every single variable, we might drop all variables except the three we think will best predict what the U.S.’s gross domestic product will look like. Advantages of feature elimination methods include simplicity and maintaining interpretability of your variables.\n",
    "\n",
    "As a disadvantage, though, you gain no information from those variables you’ve dropped. If we only use last year’s GDP, the proportion of the population in manufacturing jobs per the most recent American Community Survey numbers, and unemployment rate to predict this year’s GDP, we’re missing out on whatever the dropped variables could contribute to our model. By eliminating features, we’ve also entirely eliminated any benefits those dropped variables would bring.\n",
    "\n",
    "**Feature extraction**, however, doesn’t run into this problem. Say we have ten independent variables. In feature extraction, we create ten \"new\" independent variables, where each \"new\" independent variable is a combination of each of the ten \"old\" independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.\n",
    "\n",
    "You might say, \"Where does the dimensionality reduction come into play?\" Well, we keep as many of the new independent variables as we want, but we drop the \"least important ones.\" Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important. But because these new independent variables are combinations of our old ones, we’re still keeping the most valuable parts of our old variables, even when we drop one or more of these \"new\" variables!\n",
    "\n",
    "**Principal component analysis** is a technique for **feature extraction** — so it combines our input variables in a specific way, then we can drop the \"least important\" variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the \"new\" variables after PCA are all independent of one another.\n",
    "\n",
    "[[Reference]](#StopShopforPrincipalComponentAnalysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are principal components? How many components are there (as a function of dimensions of the original space)?\n",
    "\n",
    "### What are principal components?\n",
    "\n",
    "**Principal components** are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on. [[Reference]](#ASTEP-BY-STEPEXPLANATIONOFPRINCIPALCOMPONENTANALYSIS)\n",
    "\n",
    "There are as many principal components as there are variables. [[Reference]](#Makingsenseofprincipalcomponentanalysiseigenvectorseigenvalues)\n",
    "\n",
    "<img src=\"./resources/Principal Component Analysis Principal Components.png\" width=\"700\"/>\n",
    "\n",
    "<div style=\"text-align: center\">Percentage of Variance (Information) for each by PC</div>\n",
    "\n",
    "For this new property, with maximum possible information, called \"**first principal component**\" and for the other **principal compontents**, it is accepted that instead of saying _\"property\"/\"properties\"_ or _\"characteristic\"/\"characteristics\"_ we usually say _\"feature\"/\"features\"_ or _\"variable\"/\"variables\"_. [[Reference]](#Makingsenseofprincipalcomponentanalysiseigenvectorseigenvalues)\n",
    "\n",
    "**Principal components** (PC) basically refer to the new variables constructed as a linear combination of initial features, such that these new variables are uncorrelated. Since the principal components are independent of one another, they are perpendicular to each other in the cartesian space. [[Reference]](#UnderstandingtheMathematicsbehindPrincipalComponentAnalysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Principal Components** of a dataset are the **eigenvectors** of that dataset.\n",
    "The **eigenvectors** (**principal components**) determine the directions of the new feature space, and the **eigenvalues** determine their magnitude. [[Reference]](#PrincipalComponentAnalysisin3SimpleSteps)\n",
    "\n",
    "To get the principal components in the order of their significance, we need to rank them in the order of their **eigenvalues**. [[Reference]](#UnderstandingtheMathematicsbehindPrincipalComponentAnalysis)\n",
    "\n",
    "Every **principal component** will always be **orthogonal** (the official math term for **perpendicular**) to every other **principal component**. [[Reference]](#AOne-StopShopforPrincipalComponentAnalysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is variance? What is explained variance?\n",
    "\n",
    "To understand **variance** and **explained variance**, first let's get the basic idea of the following:\n",
    "\n",
    "- _Spread of a data set_\n",
    "\n",
    "In statistics, spread describes the variability of a data set, that is, how the data is spread out and varies from the mean.\n",
    "Measures of spread include the range; quartiles, deciles, percentiles; the five number summary; standard deviation and **variance**. \n",
    "\n",
    "[[Reference]](#AmathsdictionaryforkidsSpread)\n",
    "\n",
    "- _Standard deviation_\n",
    "\n",
    "In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values.\n",
    "A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range.\n",
    "\n",
    "Standard deviation may be abbreviated **SD**, and is most commonly represented in mathematical texts and equations by the lower case Greek letter sigma $σ$, for the population standard deviation, or the Latin letter $s$, for the sample standard deviation.\n",
    "\n",
    "[[Reference]](#Standarddeviation)\n",
    "\n",
    "- _Variance_\n",
    "\n",
    "In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its mean. In other words, it measures how far a set of numbers is spread out from their average value.\n",
    "\n",
    "[[Reference]](#Variance)\n",
    "\n",
    "- _Explained Variance_\n",
    "\n",
    "In statistics, explained variation measures the proportion to which a mathematical model accounts for the variation of a given data set. Often, variation is quantified as **variance**; then, the more specific term **explained variance** can be used.\n",
    "\n",
    "[[Reference]](#Explainedvariation)\n",
    "\n",
    "#### <u>Variance</u>\n",
    "\n",
    "In the context of PCA **variance** means explanatory power.\n",
    "PCA helps for high dimensional data by letting you see in which directions your data has the most **variance**. These directions are the **principal components**. [[Reference]](#Makingsenseofprincipalcomponentanalysiseigenvectorseigenvalues)\n",
    "\n",
    "**Eigenvalues** are the magnitude (the **variance**) in the principal components.\n",
    "The sum of all eigenvalues is equal to the sum of variances which are on the diagonal of the variance-covariance matrix.\n",
    "[[Reference]](#Makingsenseofprincipalcomponentanalysiseigenvectorseigenvalues)\n",
    "\n",
    "\n",
    "\n",
    "#### <u>Explained Variance</u>\n",
    "\n",
    "**Explained variance** can be calculated from the eigenvalues. The explained variance tells us how much information (**variance**) can be attributed to each of the principal components. [[Reference]](#PrincipalComponentAnalysisin3SimpleSteps)\n",
    "\n",
    "The **total variance** is the sum of variances of all individual principal components.\n",
    "\n",
    "The fraction of **variance explained** by a principal component is the ratio between the **variance of that principal component** and the **total variance**.\n",
    "\n",
    "For several principal components, add up their **variances** and divide by the **total variance**.\n",
    "\n",
    "[[Reference]](#ExplainedvarianceinPCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do principal components relate to explained variance?\n",
    "\n",
    "**Explained variance** can be calculated from the eigenvalues. The explained variance tells us how much information (**variance**) can be attributed to each of the principal components. [[Reference]](#PrincipalComponentAnalysisin3SimpleSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is PCA implemented? Implement and show.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and Dimensionality Reduction\n",
    "\n",
    "Often, the desired goal is to reduce the dimensions of a d-dimensional dataset by projecting it onto a ($k$)-dimensional subspace (where $k<d$) in order to increase the computational efficiency while retaining most of the information. An important question is \"what is the size of $k$ that represents the data 'well'?\"\n",
    "\n",
    "Later, we will compute **eigenvectors** (**the principal components**) of a dataset and collect them in a projection matrix. Each of those **eigenvectors** is associated with an **eigenvalue** which can be interpreted as the \"length\" or \"magnitude\" of the corresponding **eigenvector**. If some **eigenvalues** have a significantly larger magnitude than others, then the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the \"less informative\" eigenpairs is reasonable.\n",
    "\n",
    "### A Summary of the PCA Approach\n",
    "\n",
    "- Standardize the data.\n",
    "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix.\n",
    "- Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace ($k≤d$).\n",
    "- Construct the projection matrix $\\mathbf{W}$ from the selected $k$ eigenvectors.\n",
    "- Transform the original dataset $\\mathbf{X}$ via $\\mathbf{W}$ to obtain a $k$-dimensional feature subspace $\\mathbf{Y}$.\n",
    "\n",
    "### Preparing the Iris Dataset\n",
    "\n",
    "#### About Iris\n",
    "For the following tutorial, we will be working with the famous “Iris” dataset that has been deposited on the UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The three classes in the Iris dataset are:\n",
    "\n",
    "1. Iris-setosa (n=50)\n",
    "2. Iris-versicolor (n=50)\n",
    "3. Iris-virginica (n=50)\n",
    "\n",
    "And the four features of in Iris dataset are:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "<img src=\"./resources/iris.png\" alt=\"Iris\" style=\"width:720px;\"/>\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "In order to load the Iris data, we are going to use the pandas library.\n",
    "\n",
    "- We can either read it directly from the UCI repository, like so:\n",
    "\n",
    "[[Reference]](#PrincipalComponentAnalysisin3SimpleSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_len</th>\n",
       "      <th>sepal_wid</th>\n",
       "      <th>petal_len</th>\n",
       "      <th>petal_wid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_len  sepal_wid  petal_len  petal_wid           class\n",
       "145        6.7        3.0        5.2        2.3  Iris-virginica\n",
       "146        6.3        2.5        5.0        1.9  Iris-virginica\n",
       "147        6.5        3.0        5.2        2.0  Iris-virginica\n",
       "148        6.2        3.4        5.4        2.3  Iris-virginica\n",
       "149        5.9        3.0        5.1        1.8  Iris-virginica"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or download the `iris.data` file and read it locally, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_len</th>\n",
       "      <th>sepal_wid</th>\n",
       "      <th>petal_len</th>\n",
       "      <th>petal_wid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_len  sepal_wid  petal_len  petal_wid           class\n",
       "145        6.7        3.0        5.2        2.3  Iris-virginica\n",
       "146        6.3        2.5        5.0        1.9  Iris-virginica\n",
       "147        6.5        3.0        5.2        2.0  Iris-virginica\n",
       "148        6.2        3.4        5.4        2.3  Iris-virginica\n",
       "149        5.9        3.0        5.1        1.8  Iris-virginica"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='./resources/data/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dc0bb6d7a3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split data table into data X and class labels y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ix'"
     ]
    }
   ],
   "source": [
    "# split data table into data X and class labels y\n",
    "\n",
    "X = df.ix[:,0:4].values\n",
    "y = df.ix[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data table into data X and class labels y\n",
    "\n",
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our iris dataset is now stored in form of a  $150 \\times 4$ matrix where the columns are the different features, and every row represents a separate flower sample.\n",
    "Each sample row $\\mathbf{x}$ can be pictured as a 4-dimensional vector   \n",
    "\n",
    "\n",
    "$\\mathbf{x^T} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\text{sepal length} \\\\ \\text{sepal width} \\\\\\text{petal length} \\\\ \\text{petal width} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some applications of PCA, e.g. reducing a 3D image to its first 2 principal components, plotting the 3D and 2D images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a practical use of PCA, for example, trying to see features in a 15D space, projected in 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "<a id=\"Definitions\"></a>\n",
    "\n",
    "### Euclidean vector\n",
    "[[Euclidean vector]](#Euclideanvector)\n",
    "\n",
    "In mathematics, physics and engineering, a Euclidean vector or simply a vector (sometimes called a geometric vector or spatial vector) is a geometric object that has magnitude (or length) and direction. Vectors can be added to other vectors according to vector algebra. A Euclidean vector is frequently represented by a ray (a line segment with a definite direction), or graphically as an arrow connecting an initial point $A$ with a terminal point $B$, and denoted by $\\overrightarrow{AB}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.annotate('A', xy=(-0.3, -0.3))\n",
    "ax.annotate('B', xy=(2.1, 3.1))\n",
    "ax.text(0.5, 1.5, '$\\overrightarrow{a}$')\n",
    "\n",
    "plt.quiver(0, 0, 2, 3, scale_units = \"xy\", angles = \"xy\", scale = 1, color ='b')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "# DO NOT USE plt.axis(\"equal\") - will not work, or worse will not ALWAYS work, use plt.gca().set_aspect(\"equal\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant of a square matrix\n",
    "[[Determinant]](#Determinant)\n",
    "\n",
    "In mathematics, the determinant is a scalar value that is a function of the entries of a square matrix. It allows characterizing some properties of the matrix and the linear map represented by the matrix. In particular, the determinant is nonzero if and only if the matrix is **invertible**, and the linear map represented by the matrix is an isomorphism. The determinant of a product of matrices is the product of their determinants (the preceding property is a corollary of this one). The determinant of a matrix A is denoted $det(A)$, $det A$, or $|A|$.\n",
    "\n",
    "In the case of a $2 \\times 2$ matrix the determinant can be defined as:\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}|A|={\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}}=ad-bc.\\end{aligned}}}$$\n",
    "\n",
    "Similarly, for a 3 × 3 matrix A, its determinant is\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}|A|={\\begin{vmatrix}a&b&c\\\\d&e&f\\\\g&h&i\\end{vmatrix}}&=a\\,{\\begin{vmatrix}e&f\\\\h&i\\end{vmatrix}}-b\\,{\\begin{vmatrix}d&f\\\\g&i\\end{vmatrix}}+c\\,{\\begin{vmatrix}d&e\\\\g&h\\end{vmatrix}}\\\\[3pt]&=aei+bfg+cdh-ceg-bdi-afh.\\end{aligned}}}$$\n",
    "\n",
    "Each determinant of a 2 × 2 matrix in this equation is called a minor of the matrix A. \n",
    "\n",
    "#### Determinant of a square matrix definition\n",
    "\n",
    "There are various equivalent ways to define the determinant of a square matrix $A$, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective **minors**; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a $4 × 4$ matrix:\n",
    "\n",
    "$${\\displaystyle {\\begin{vmatrix}a&b&c&d\\\\e&f&g&h\\\\i&j&k&l\\\\m&n&o&p\\end{vmatrix}}=a\\,{\\begin{vmatrix}f&g&h\\\\j&k&l\\\\n&o&p\\end{vmatrix}}-b\\,{\\begin{vmatrix}e&g&h\\\\i&k&l\\\\m&o&p\\end{vmatrix}}+c\\,{\\begin{vmatrix}e&f&h\\\\i&j&l\\\\m&n&p\\end{vmatrix}}-d\\,{\\begin{vmatrix}e&f&g\\\\i&j&k\\\\m&n&o\\end{vmatrix}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor\n",
    "[[Minor]](#Minorlinearalgebra)\n",
    "\n",
    "In linear algebra, a minor of a matrix $A$ is the determinant of some smaller square matrix, cut down from A by removing one or more of its rows and columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear isomorphism a.k.a. Linear map a.k.a. Linear transformation\n",
    "[[Linear map]](#Linearmap)\n",
    "\n",
    "In mathematics, a linear map (also called a **linear mapping, linear transformation, vector space homomorphism**, or in some contexts **linear function**) is a mapping ${\\displaystyle V\\rightarrow W}$ between two vector spaces that preserves the operations of vector addition and scalar multiplication.\n",
    "If a linear map is a bijection then it is called a **linear isomorphism**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bijection\n",
    "[[Bijection]](#Bijectionwikipedia)\n",
    "\n",
    "In mathematics, a bijection, bijective function, one-to-one correspondence, or invertible function, is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. There are no unpaired elements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear independence\n",
    "[[Linear independence]](#Linearindependence)\n",
    "\n",
    "In the theory of vector spaces, a set of vectors is said to be linearly dependent if at least one of the vectors in the set can be defined as a **linear combination** of the others; if no vector in the set can be written in this way, then the vectors are said to be **linearly independent**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear combination\n",
    "[[Linear combination]](#Linearcombination)\n",
    "    \n",
    "In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of x and y would be any expression of the form $ax + by$, where a and b are constants).\n",
    "\n",
    "Let $V$ be a vector space over the field $K$. As usual, we call elements of $V$ **vectors** and call elements of $K$ **scalars**. If $v_1,...,v_n$ are vectors and $a_1,...,a_n$ are scalars, then the linear combination of those vectors with those scalars as coefficients is:\n",
    "\n",
    "$${\\displaystyle a_{1}\\mathbf {v} _{1}+a_{2}\\mathbf {v} _{2}+a_{3}\\mathbf {v} _{3}+\\cdots +a_{n}\\mathbf {v} _{n}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-determinant\n",
    "[[Pseudo-determinant]](#Pseudodeterminant)\n",
    "\n",
    "In linear algebra and statistics, the **pseudo-determinant** is the product of all non-zero eigenvalues of a square matrix. It coincides with the regular determinant when the matrix is **non-singular**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invertible matrix\n",
    "[[Invertible matrix]](#Invertiblematrix)\n",
    "\n",
    "In linear algebra, an $n-by-n$ square matrix $A$ is called invertible (also **nonsingular** or **nondegenerate**), if there exists an $n-by-n$ square matrix B such that:\n",
    "\n",
    "$${\\displaystyle \\mathbf {AB} =\\mathbf {BA} =\\mathbf {I} _{n}\\ }$$\n",
    "\n",
    "where $I_n$ denotes the $n-by-n$ identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix $B$ is uniquely determined by $A$, and is called the (multiplicative) **inverse** of $A$, denoted by $A^{−1}$. Matrix inversion is the process of finding the matrix $B$ that satisfies the prior equation for a given **invertible** matrix $A$.\n",
    "\n",
    "A square matrix that is **not invertible** is called **singular** or **degenerate**. A square matrix is **singular** _if and only if_ its determinant is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristic polynomial\n",
    "[[Characteristic polynomial]](#Characteristicpolynomial)\n",
    "\n",
    "In linear algebra, the **characteristic polynomial** of a square matrix is a polynomial which is invariant under matrix similarity and **has the eigenvalues as roots**. It has the **determinant** and the **trace** of the matrix among its coefficients.\n",
    "\n",
    "#### Characteristic polynomial motivation\n",
    "Given a square matrix $A$, we want to find a polynomial whose zeros are the eigenvalues of $A$. For a **diagonal matrix** $A$, the characteristic polynomial is easy to define: if the diagonal entries are $a_1, a_2, a_3, etc.$ then the characteristic polynomial will be:\n",
    "\n",
    "$${\\displaystyle (t-a_{1})(t-a_{2})(t-a_{3})\\cdots}$$\n",
    "\n",
    "This works because the diagonal entries are also the eigenvalues of this matrix.\n",
    "\n",
    "For a general matrix A, one can proceed as follows. A scalar $\\lambda$ is an eigenvalue of $A$ if and only if there is a nonzero vector $v$, called an eigenvector, such that:\n",
    "\n",
    "$${\\displaystyle A\\mathbf {v} =\\lambda \\mathbf {v}}$$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$${\\displaystyle (\\lambda I-A)\\mathbf {v} =0}$$\n",
    "\n",
    "(where $I$ is the identity matrix). Since $v$ must be nonzero, this means that the matrix $λI – A$ has a nonzero **kernel**. Thus this matrix is not **invertible**, and the same is true for its **determinant**, which must therefore be zero. Thus the eigenvalues of $A$ are the roots of $det(λI – A)$, which is a polynomial in $λ$.\n",
    "\n",
    "#### Characteristic polynomial formal definition\n",
    "\n",
    "We consider an $n×n$ matrix $A$. The characteristic polynomial of $A$, denoted by $p_A(t)$, is the polynomial defined by:\n",
    "\n",
    "${\\displaystyle p_{A}(t)=\\det \\left(tI-A\\right)}$$\n",
    "\n",
    "where $I$ denotes the $n×n$ identity matrix.\n",
    "\n",
    "### Trace\n",
    "[[Trace]](#Tracelinearalgebra)\n",
    "\n",
    "In linear algebra, the trace of a square matrix $A$, denoted $tr(A)$, is defined to be the sum of elements on the main diagonal (from the upper left to the lower right) of $A$.\n",
    "\n",
    "The trace of a matrix is the sum of its (complex) eigenvalues (counted with multiplicities), and it is invariant with respect to a change of basis. The trace is only defined for a square matrix $(n × n)$.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "The trace of an $n × n$ square matrix $A$ is defined as:\n",
    "\n",
    "$${\\displaystyle \\operatorname {tr} (\\mathbf {A} )=\\sum _{i=1}^{n}a_{ii}=a_{11}+a_{22}+\\dots +a_{nn}}$$\n",
    "\n",
    "where $a_{ii}$ denotes the entry on the $ith$ row and $ith$ column of $A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel (linear algebra)\n",
    "[[Kernel]](#Kernellinearalgebra)\n",
    "\n",
    "In mathematics, the kernel of a linear map, also known as the **null space** or **nullspace**, is the linear subspace of the domain of the map which is mapped to the **zero vector**. That is, given a linear map $L : V → W$ between two vector spaces $V$ and $W$, the kernel of $L$ is the vector space of all elements $v$ of $V$ such that $L(v) = 0$, where $0$ denotes the zero vector in $W$, or more symbolically:\n",
    "\n",
    "$${\\displaystyle \\ker(L)=\\left\\{\\mathbf {v} \\in V\\mid L(\\mathbf {v} )=\\mathbf {0} \\right\\}}$$\n",
    "\n",
    "<img src=\"./resources/KerIm_2015Joz_L2.png\" width=\"250\" height=\"150\"/>\n",
    "\n",
    "### Linear subspace\n",
    "[[Linear subspace]](#Linearsubspace)\n",
    "\n",
    "In mathematics, and more specifically in linear algebra, a **linear subspace**, also known as a **vector subspace** is a vector space that is a subset of some larger vector space. A linear subspace is usually simply called a subspace when the context serves to distinguish it from other types of subspaces.\n",
    "\n",
    "### Diagonal matrix\n",
    "[[Diagonal matrix]](#Diagonalmatrix)\n",
    "\n",
    "In linear algebra, a diagonal matrix is a matrix in which the entries outside the main diagonal are all zero; the term usually refers to square matrices. An example of a $2×2$ diagonal matrix is ${\\displaystyle \\left[{\\begin{smallmatrix}3&0\\\\0&2\\end{smallmatrix}}\\right]}$, while an example of a $3×3$ diagonal matrix is ${\\displaystyle \\left[{\\begin{smallmatrix}6&0&0\\\\0&7&0\\\\0&0&4\\end{smallmatrix}}\\right]}$. An identity matrix of any size, or any multiple of it (a scalar matrix), is a diagonal matrix.\n",
    "\n",
    "A diagonal matrix is sometimes called a scaling matrix, since matrix multiplication with it results in changing scale (size). Its determinant is the product of its diagonal values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero vector\n",
    "[[Zero vector]](#Zeroelement)\n",
    "\n",
    "The zero vector under vector addition: the vector of length 0 and whose components are all 0. Often denoted as ${\\displaystyle \\mathbf{0}}$  or ${\\displaystyle {\\vec {0}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank (linear algebra)\n",
    "[[Rank (linear algebra)]](#Ranklinearalgebra)\n",
    "\n",
    "In linear algebra, the rank of a matrix $A$ is the dimension of the vector space generated (or spanned) by its columns. This corresponds to the maximal number of **linearly independent** columns of $A$. This, in turn, is identical to the dimension of the vector space spanned by its rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal matrix\n",
    "\n",
    "[[Orthogonal matrix]](#Orthogonalmatrix)\n",
    "\n",
    "In linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are **orthonormal vectors**.\n",
    "\n",
    "One way to express this is\n",
    "\n",
    "${\\displaystyle Q^{\\mathrm {T} }Q=QQ^{\\mathrm {T} }=I}$, \n",
    "\n",
    "where $QT$ is the transpose of $Q$ and $I$ is the identity matrix.\n",
    "\n",
    "This leads to the equivalent characterization: a matrix $Q$ is orthogonal if its transpose is equal to its inverse:\n",
    "\n",
    "${\\displaystyle Q^{\\mathrm {T} }=Q^{-1}}$,\n",
    "\n",
    "where $Q−1$ is the inverse of $Q$.\n",
    "\n",
    "An orthogonal matrix $Q$ is necessarily invertible (with inverse $Q−1 = QT$), unitary ($Q−1 = Q∗$),where $Q∗$ is the Hermitian adjoint (conjugate transpose) of $Q$, and therefore normal ($Q∗Q = QQ∗$) over the real numbers. The determinant of any orthogonal matrix is either $+1$ or $−1$. As a linear transformation, an orthogonal matrix preserves the inner product of vectors, and therefore acts as an isometry of Euclidean space, such as a rotation, reflection or rotoreflection. In other words, it is a unitary transformation.\n",
    "\n",
    "The set of $n × n$ orthogonal matrices forms a group, $O(n)$, known as the orthogonal group. The subgroup $SO(n)$ consisting of orthogonal matrices with determinant $+1$ is called the **special orthogonal group**, and each of its elements is a **special orthogonal matrix**. As a linear transformation, every special orthogonal matrix acts as a **rotation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthonormality\n",
    "[[Orthonormality]](#Orthonormality)\n",
    "\n",
    "In linear algebra, two vectors in an inner product space are **orthonormal** if they are orthogonal (or perpendicular along a line) unit vectors. A set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length. An orthonormal set which forms a basis is called an orthonormal basis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilbert space\n",
    "[[Hilbert space]](#Hilbertspace)\n",
    "\n",
    "The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is a vector space equipped with an inner product, an operation that allows lengths and angles to be defined. Furthermore, Hilbert spaces are complete, which means that there are enough limits in the space to allow the techniques of calculus to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner product space\n",
    "[[Inner product space]](#Innerproductspace)\n",
    "\n",
    "In mathematics, an **inner product space** or a **Hausdorff pre-Hilbert space** is a vector space with a binary operation called an **inner product**. This operation associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors, often denoted using angle brackets (as in ${\\displaystyle \\langle a,b\\rangle }$). Inner products allow the rigorous introduction of intuitive geometrical notions, such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete metric space\n",
    "[[Complete metric space]](#Completemetricspace)\n",
    "\n",
    "In mathematical analysis, a metric space $M$ is called **complete** (or a **Cauchy space**) if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.\n",
    "\n",
    "Intuitively, a space is complete if there are no \"points missing\" from it (inside or at the boundary). For instance, the set of rational numbers is not complete, because e.g. ${\\displaystyle {\\sqrt {2}}}$ is \"missing\" from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit (mathematics)\n",
    "[[Limit (mathematics)]](#Limitmathematics)\n",
    "\n",
    "In mathematics, a **limit** is the value that a function (or sequence) \"approaches\" as the input (or index) \"approaches\" some value.\n",
    "\n",
    "In formulas, a limit of a function is usually written as:\n",
    "\n",
    "$${\\displaystyle \\lim _{x\\to c}f(x)=L}$$\n",
    "\n",
    "and is read as \"the limit of $f$ of $x$ as $x$ approaches $c$ equals $L$\". The fact that a function $f$ approaches the limit $L$ as $x$ approaches $c$ is sometimes denoted by a right arrow ($→$ or ${\\displaystyle \\rightarrow }$), as in:\n",
    "\n",
    "$${\\displaystyle f(x)\\to L{\\text{ as }}x\\to c}$$\n",
    "\n",
    "which reads \"${\\displaystyle f}$ of ${\\displaystyle x}$ tends to ${\\displaystyle L}$ as ${\\displaystyle x}$ tends to ${\\displaystyle c}$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit vector\n",
    "\n",
    "[[Unit vector]](#Unitvector)\n",
    "\n",
    "The term direction vector is used to describe a unit vector being used to represent spatial direction, and such quantities are commonly denoted as $\\pmb{d}$; $2D$ spatial directions represented this way are numerically equivalent to points on the unit circle. The same construct is used to specify spatial directions in $3D$, which are equivalent to a point on the unit sphere.\n",
    "\n",
    "The **normalized vector** $\\hat{u}$ of a non-zero vector $\\pmb{u}$ is the **unit vector** in the direction of $u$, i.e.,\n",
    "\n",
    "$${\\displaystyle \\mathbf {\\hat {u}} ={\\frac {\\mathbf {u} }{|\\mathbf {u} |}}}$$\n",
    "\n",
    "where $|\\pmb{u}|$ is the norm (or length) of $\\pmb{u}$. The term **normalized vector** is sometimes used as a synonym for **unit vector**.\n",
    "\n",
    "Unit vectors are often chosen to form the basis of a vector space, and every vector in the space may be written as a linear combination of unit vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normed vector space\n",
    "\n",
    "[[Normed vector space]](#Normedvectorspace)\n",
    "\n",
    "In mathematics, a normed vector space or normed space is a vector space over the real or complex numbers, on which a norm is defined. A norm is the formalization and the generalization to real vector spaces of the intuitive notion of \"length\" in the real world. A norm is a real-valued function defined on the vector space that is commonly denoted ${\\displaystyle x\\mapsto \\|x\\|}$, and has the following properties:\n",
    "\n",
    "1. It is nonnegative, that is for every vector $x$, one has \n",
    "$${\\displaystyle \\|x\\|\\geq 0}$$\n",
    "\n",
    "2. It is positive on nonzero vectors, that is,\n",
    "$${\\displaystyle \\|x\\|=0\\Longleftrightarrow x=0.}$$\n",
    "\n",
    "3. For every vector $x$, and every scalar ${\\displaystyle \\alpha}$, one has\n",
    "$${\\displaystyle \\|\\alpha x\\|=|\\alpha |\\|x\\|}$$\n",
    "\n",
    "4. The triangle inequality holds; that is, for every vectors $x$ and $y$, one has\n",
    "${\\displaystyle \\|x+y\\|\\leq \\|x\\|+\\|y\\|}$\n",
    "\n",
    "A norm induces a distance by the formula:\n",
    "\n",
    "$${\\displaystyle d(x,y)=\\|y-x\\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm (mathematics) \n",
    "\n",
    "[[Norm (mathematics)]](#Normmathematics)\n",
    "\n",
    "In mathematics, a **norm** is a function from a real or complex vector space to the nonnegative real numbers that behaves in certain ways like the distance from the origin: it commutes with scaling, obeys a form of the triangle inequality, and is zero only at the origin. In particular, the Euclidean distance of a vector from the origin is a **norm**, called the **Euclidean norm**, or **2-norm**, which may also be defined as the square root of the inner product of a vector with itself.\n",
    "\n",
    "A **pseudonorm** or **seminorm** satisfies the first two properties of a **norm**, but may be zero for other vectors than the origin. A vector space with a specified **norm** is called a **normed vector** space. In a similar manner, a vector space with a seminorm is called a seminormed vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean space\n",
    "[[Euclidean space]](#Euclideanspace)\n",
    "\n",
    "**Euclidean space** is the fundamental space of classical geometry. Originally it was the three-dimensional space of Euclidean geometry, but in modern mathematics there are Euclidean spaces of any nonnegative integer dimension, including the three-dimensional space and the Euclidean plane (dimension two).\n",
    "\n",
    "There is essentially only one Euclidean space of each dimension; that is, all Euclidean spaces of a given dimension are isomorphic. Therefore, in many cases, it is possible to work with a specific Euclidean space, which is generally the real $n-space$ ${\\displaystyle \\mathbb {R} ^{n}}$, equipped with the dot product. An isomorphism from a Euclidean space to ${\\displaystyle \\mathbb {R} ^{n}}$ associates with each point an $n-tuple$ of real numbers which locate that point in the Euclidean space and are called the Cartesian coordinates of that point.\n",
    "\n",
    "<img src=\"./resources/Coord_system_CA_0.svg\" width=\"248\"/>\n",
    "\n",
    "<div style=\"text-align: center\">A point in three-dimensional Euclidean space can be located by three coordinates.</div>\n",
    "\n",
    "Technical definition\n",
    "\n",
    "A **Euclidean vector space** is a finite-dimensional inner product space over the real numbers.\n",
    "\n",
    "A **Euclidean space** is an affine space over the real numbers such that the associated vector space is a **Euclidean vector space**. **Euclidean spaces** are sometimes called **Euclidean affine spaces** for distinguishing them from **Euclidean vector spaces**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "\n",
    "[[Euclidean distance]](#Euclideandistance)\n",
    "\n",
    "In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance. \n",
    "\n",
    "#### Distance formulas\n",
    "\n",
    "***One dimension***\n",
    "\n",
    "The distance between any two points on the real line is the absolute value of the numerical difference of their coordinates. Thus if ${\\displaystyle p}$ and ${\\displaystyle q}$ are two points on the real line, then the distance between them is given by:\n",
    "\n",
    "${\\displaystyle d(p,q)=|p-q|}$\n",
    "\n",
    "A more complicated formula, giving the same value, but generalizing more readily to higher dimensions, is:\n",
    "\n",
    "${\\displaystyle d(p,q)={\\sqrt {(p-q)^{2}}}}$\n",
    "\n",
    "In this formula, squaring and then taking the square root leaves any positive number unchanged, but replaces any negative number by its absolute value.\n",
    "\n",
    "***Two dimensions***\n",
    "\n",
    "In the Euclidean plane, let point ${\\displaystyle p}$ have Cartesian coordinates ${\\displaystyle (p_{1},p_{2})}$ and let point ${\\displaystyle q}$ have coordinates ${\\displaystyle (q_{1},q_{2})}$. Then the distance between ${\\displaystyle p}$ and ${\\displaystyle q}$ is given by:\n",
    "\n",
    "${\\displaystyle d(p,q)={\\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}}}}$\n",
    "\n",
    "This can be seen by applying the Pythagorean theorem to a right triangle with horizontal and vertical sides, having the line segment from ${\\displaystyle p}$ to ${\\displaystyle q}$ as its hypotenuse. The two squared formulas inside the square root give the areas of squares on the horizontal and vertical sides, and the outer square root converts the area of the square on the hypotenuse into the length of the hypotenuse.\n",
    "\n",
    "It is also possible to compute the distance for points given by polar coordinates. If the polar coordinates of ${\\displaystyle p}$ are ${\\displaystyle (r,\\theta )}$ and the polar coordinates of ${\\displaystyle q}$ are ${\\displaystyle (s,\\psi )}$, then their distance is:\n",
    "\n",
    "${\\displaystyle d(p,q)={\\sqrt {r^{2}+s^{2}-2rs\\cos(\\theta -\\psi )}}}$\n",
    "\n",
    "When ${\\displaystyle p}$ and ${\\displaystyle q}$ are expressed as complex numbers in the complex plane, the same formula for one-dimensional points expressed as real numbers can be used:\n",
    "\n",
    "${\\displaystyle d(p,q)=|p-q|.}$\n",
    "\n",
    "***Higher dimensions***\n",
    "\n",
    "Deriving the ${\\displaystyle n}$-dimensional Euclidean distance formula by repeatedly applying the Pythagorean theorem\n",
    "\n",
    "In three dimensions, for points given by their Cartesian coordinates, the distance is:\n",
    "\n",
    "${\\displaystyle d(p,q)={\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+(p_{3}-q_{3})^{2}}}}$\n",
    "\n",
    "In general, for points given by Cartesian coordinates in ${\\displaystyle n}$-dimensional Euclidean space, the distance is:\n",
    "\n",
    "${\\displaystyle d(p,q)={\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{i}-q_{i})^{2}+\\cdots +(p_{n}-q_{n})^{2}}}}$\n",
    "\n",
    "\n",
    "<img src=\"./resources/260px-Euclidean_distance_3d_2_cropped.png\" width=\"260\"/>\n",
    "\n",
    "<div style=\"text-align: center\">Deriving the ${\\displaystyle n}$-dimensional Euclidean distance formula by repeatedly applying the Pythagorean theorem.</div>\n",
    "\n",
    "***Objects other than points***\n",
    "\n",
    "For pairs of objects that are not both points, the distance can most simply be defined as the smallest distance between any two points from the two objects, although more complicated generalizations from points to sets such as Hausdorff distance are also commonly used. Formulas for computing distances between different types of objects include:\n",
    "\n",
    "- The distance from a point to a line, in the Euclidean plane [[Distance from a point to a line]](#Distancefromapointtoaline)\n",
    "- The distance from a point to a plane in three-dimensional Euclidean space [[Distance from a point to a plane]](#Distancefromapointtoaplane)\n",
    "- The distance between two lines in three-dimensional Euclidean space [[Skew lines]](#Skewlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine space\n",
    "\n",
    "[[Affine space]](#Affinespace)\n",
    "\n",
    "In mathematics, an **affine space** is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.\n",
    "\n",
    "In an **affine space**, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an **affine space**, there are instead displacement vectors, also called translation vectors or simply translations, between two points of the space. Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an **affine space**, resulting in a new point translated from the starting point by that vector.\n",
    "\n",
    "Any vector space may be viewed as an **affine space**; this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as points of the **affine space** or as displacement vectors or translations. When considered as a point, the zero vector is called the origin. Adding a fixed vector to the elements of a linear subspace of a vector space produces an affine subspace. One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomorphism\n",
    "[[Isomorphism]](#Isomorphism)\n",
    "\n",
    "In mathematics, an **isomorphism** is a structure-preserving mapping between two structures of the same type that can be reversed by an inverse mapping. Two mathematical structures are isomorphic if an isomorphism exists between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous function\n",
    "\n",
    "[[Homogeneousfunction]](#Homogeneousfunction)\n",
    "\n",
    "In mathematics, a **homogeneous function** is one with multiplicative scaling behaviour: if all its arguments are multiplied by a factor, then its value is multiplied by some power of this factor.\n",
    "\n",
    "For example, a homogeneous real-valued function of two variables $x$ and $y$ is a real-valued function that satisfies the condition ${\\displaystyle f(\\alpha x,\\alpha y)=\\alpha ^{k}f(x,y)}$ for some constant $k$ and all real numbers $α$. The constant $k$ is called the degree of homogeneity.\n",
    "\n",
    "More generally, if $ƒ : V → W$ is a function between two vector spaces over a field $F$, and $k$ is an integer, then $ƒ$ is said to be homogeneous of degree $k$ if:\n",
    "\n",
    "${\\displaystyle f(\\alpha \\mathbf {v} )=\\alpha ^{k}f(\\mathbf {v} )}$ \n",
    "\n",
    "for all nonzero $α ∈ F$ and $v ∈ V$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map projection\n",
    "\n",
    "[[Map projection]](#Mapprojection)\n",
    "\n",
    "In cartography, a **map projection** is a way to flatten a globe's surface into a plane in order to make a map. This requires a systematic transformation of the latitudes and longitudes of locations from the surface of the globe into locations on a plane. All projections of a sphere on a plane necessarily distort the surface in some way and to some extent. Depending on the purpose of the map, some distortions are acceptable and others are not; therefore, different map projections exist in order to preserve some properties of the sphere-like body at the expense of other properties. The study of map projections is the characterization of the distortions. There is no limit to the number of possible map projections. Projections are a subject of several pure mathematical fields, including differential geometry, projective geometry, and manifolds. However, \"map projection\" refers specifically to a cartographic projection.\n",
    "\n",
    "Despite the name's literal meaning, projection is not limited to perspective projections, such as those resulting from casting a shadow on a screen, or the rectilinear image produced by a pinhole camera on a flat film plate. Rather, any mathematical function that transforms coordinates from the curved surface distinctly and smoothly to the plane is a projection. Few projections in practical use are perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "- PCA - Principal Component Analysis\n",
    "- 3D - Three Dimensional\n",
    "- 2D - Two Dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "### PCA\n",
    "\n",
    "### A Tutorial on Principal Component Analysis, Jonathon Shlens, 7 April 2014; Version 3.02 <a id=2>[1]</a>\n",
    "\n",
    "<https://arxiv.org/pdf/1404.1100.pdf>\n",
    "\n",
    "### A Tutorial on Principal Component Analysis, Jonathon Shlens, 10 December 2005; Version 2 <a id=2>[2]</a>\n",
    "\n",
    "<https://www.cs.cmu.edu/~tom/10701_sp11/slides/pca_schlens.pdf>\n",
    "\n",
    "### A Tutorial on Principal Component Analysis, Jonathon Shlens, 25 March 2003; Version 1 <a id=2>[3]</a>\n",
    "\n",
    "<https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf>\n",
    "\n",
    "### A One-Stop Shop for Principal Component Analysis <a id=\"AOne-StopShopforPrincipalComponentAnalysis\"></a>\n",
    "\n",
    "<https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>\n",
    "\n",
    "### The Mathematics Behind Principal Component Analysis <a id=\"TheMathematicsBehindPrincipalComponentAnalysis\"></a>\n",
    "\n",
    "<https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643>\n",
    "\n",
    "### Principal component analysis <a id=\"\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Principal_component_analysis>\n",
    "\n",
    "### A STEP-BY-STEP EXPLANATION OF PRINCIPAL COMPONENT ANALYSIS (PCA) <a id=\"ASTEP-BY-STEPEXPLANATIONOFPRINCIPALCOMPONENTANALYSIS\"></a>\n",
    "\n",
    "<https://builtin.com/data-science/step-step-explanation-principal-component-analysis>\n",
    "\n",
    "### Understanding the Mathematics behind Principal Component Analysis <a id=\"UnderstandingtheMathematicsbehindPrincipalComponentAnalysis\"></a>\n",
    "\n",
    "<https://heartbeat.fritz.ai/understanding-the-mathematics-behind-principal-component-analysis-efd7c9ff0bb3>\n",
    "\n",
    "### The Mathematics Behind Principal Component Analysis (PCA) <a id=\"\"></a>\n",
    "\n",
    "<https://medium.com/analytics-vidhya/mathematics-behind-principal-component-analysis-pca-1cdff0a808a9>\n",
    "\n",
    "### Principal Component Analysis in 3 Simple Steps <a id=\"#PrincipalComponentAnalysisin3SimpleSteps\"></a>\n",
    "\n",
    "<https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html>\n",
    "\n",
    "### A tutorial on Principal Components Analysis, Lindsay I Smith, February 26, 2002 <a id=\"\"></a>\n",
    "\n",
    "<http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf>\n",
    "\n",
    "### Making sense of principal component analysis, eigenvectors & eigenvalues <a id=\"Makingsenseofprincipalcomponentanalysiseigenvectorseigenvalues\"></a>\n",
    "\n",
    "<https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues>\n",
    "\n",
    "---\n",
    "\n",
    "### Wolframalpha\n",
    "\n",
    "<https://www.wolframalpha.com/>\n",
    "\n",
    "---\n",
    "\n",
    "### 3Blue1Brown\n",
    "\n",
    "<https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw>\n",
    "\n",
    "---\n",
    "\n",
    "### StatQuest with Josh Starmer\n",
    "\n",
    "<https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw/search?query=pca>\n",
    "\n",
    "---\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors>\n",
    "\n",
    "<https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices>\n",
    "\n",
    "### Eigenbasis\n",
    "\n",
    "<https://intuitive-math.club/linear-algebra/eigenbasis/>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix> <a id=\"1\">[1]</a>\n",
    "\n",
    "<https://canvas.harvard.edu/files/3780067/download?download_frd=1&verifier=hNHLukPIpGtkoApu6WIE51qatsQV7VNvCCak7jwW>\n",
    "\n",
    "<http://www.math.lsa.umich.edu/~kesmith/EigenEverything2017.pdf>\n",
    "\n",
    "<https://math.stackexchange.com/questions/723762/eigenspace-what-is-it>\n",
    "\n",
    "<https://math.stackexchange.com/questions/36815/a-simple-explanation-of-eigenvectors-and-eigenvalues-with-big-picture-ideas-of>\n",
    "\n",
    "### Spectrum of a matrix <a id=\"Spectrumofamatrix\"></a>\n",
    "<https://en.wikipedia.org/wiki/Spectrum_of_a_matrix> \n",
    "\n",
    "### Determinant <a id=\"Determinant\"></a>\n",
    "<https://en.wikipedia.org/wiki/Determinant>\n",
    "\n",
    "### Minor <a id=\"Minorlinearalgebra\"></a>\n",
    "<https://en.wikipedia.org/wiki/Minor_(linear_algebra)>\n",
    "\n",
    "### Linear map <a id=\"Linearmap\"></a>\n",
    "<https://en.wikipedia.org/wiki/Linear_map>\n",
    "\n",
    "### Bijection <a id=\"Bijectionwikipedia\"></a>\n",
    "<https://en.wikipedia.org/wiki/Bijection>\n",
    "\n",
    "### Pseudo-determinant <a id=\"Pseudodeterminant\"></a>\n",
    "<https://en.wikipedia.org/wiki/Pseudo-determinant>\n",
    "\n",
    "### Invertible matrix <a id=\"Invertiblematrix\"></a>\n",
    "<https://en.wikipedia.org/wiki/Invertible_matrix>\n",
    "\n",
    "### Characteristic polynomial <a id=\"Characteristicpolynomial\"></a>\n",
    "<https://en.wikipedia.org/wiki/Characteristic_polynomial>\n",
    "\n",
    "### Trace <a id=\"Tracelinearalgebra\"></a>\n",
    "<https://en.wikipedia.org/wiki/Trace_(linear_algebra)>\n",
    "\n",
    "### Diagonal matrix <a id=\"Diagonalmatrix\"></a>\n",
    "<https://en.wikipedia.org/wiki/Diagonal_matrix>\n",
    "\n",
    "### Orthogonal matrix <a id=\"Orthogonalmatrix\"></a>\n",
    "<https://en.wikipedia.org/wiki/Orthogonal_matrix>\n",
    "\n",
    "### Orthonormality <a id=\"Orthonormality\"></a>\n",
    "<https://en.wikipedia.org/wiki/Orthonormality>\n",
    "\n",
    "### Linear subspace <a id=\"Linearsubspace\"></a>\n",
    "<https://en.wikipedia.org/wiki/Linear_subspace>\n",
    "\n",
    "### Euclidean vector <a id=\"Euclideanvector\"></a>\n",
    "<https://en.wikipedia.org/wiki/Euclidean_vector>\n",
    "\n",
    "### Rank (linear algebra) <a id=\"Ranklinearalgebra\"></a>\n",
    "<https://en.wikipedia.org/wiki/Rank_(linear_algebra)>\n",
    "\n",
    "### Zero element <a id=\"Zeroelement\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Zero_element#Additive_identities>\n",
    "\n",
    "### Linear independence <a id=\"Linearindependence\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Linear_independence>\n",
    "\n",
    "### Linear combination <a id=\"Linearcombination\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Linear_combination>\n",
    "\n",
    "### Projection (mathematics) <a id=\"Projectionmathematics\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Projection_(mathematics)>\n",
    "\n",
    "### Projection (linear algebra) <a id=\"Projectionlinearalgebra\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Projection_(linear_algebra)>\n",
    "\n",
    "### Hilbert space <a id=\"Hilbertspace\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Hilbert_space>\n",
    "\n",
    "### Inner product space <a id=\"Innerproductspace\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Inner_product_space>\n",
    "\n",
    "### Complete metric space <a id=\"Completemetricspace\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Complete_metric_space>\n",
    "\n",
    "### Limit (mathematics) <a id=\"Limitmathematics\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Limit_(mathematics)>\n",
    "\n",
    "### Vector projection <a id=\"Vectorprojection\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Vector_projection>\n",
    "\n",
    "### Unit vector <a id=\"Unitvector\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Unit_vector>\n",
    "\n",
    "### Normed vector space <a id=\"Normedvectorspace\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Normed_vector_space>\n",
    "\n",
    "### Norm (mathematics) <a id=\"Normmathematics\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Norm_(mathematics)>\n",
    "\n",
    "### Euclidean space <a id=\"Euclideanspace\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Euclidean_space#Euclidean_norm>\n",
    "\n",
    "### Euclidean distance <a id=\"Euclideandistance\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Euclidean_distance>\n",
    "\n",
    "### Distance from a point to a line <a id=\"Distancefromapointtoaline\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line>\n",
    "\n",
    "### Distance from a point to a plane <a id=\"Distancefromapointtoaplane\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane>\n",
    "\n",
    "### Skew lines <a id=\"Skewlines\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Skew_lines#Distance>\n",
    "\n",
    "### Affine space <a id=\"Affinespace\"></a>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Affine_space>\n",
    "\n",
    "### Isomorphism <a id=\"Isomorphism\"></a>\n",
    "<https://en.wikipedia.org/wiki/Isomorphism>\n",
    "\n",
    "### Homogeneous function <a id=\"Homogeneousfunction\"></a>\n",
    "<https://en.wikipedia.org/wiki/Homogeneous_function>\n",
    "\n",
    "### 3D projection <a id=\"3Dprojection\"></a>\n",
    "<https://en.wikipedia.org/wiki/3D_projection>\n",
    "\n",
    "### Parallel projection <a id=\"Parallelprojection\"></a>\n",
    "<https://en.wikipedia.org/wiki/Parallel_projection>\n",
    "\n",
    "### Map projection <a id=\"Mapprojection\"></a>\n",
    "<https://en.wikipedia.org/wiki/Map_projection>\n",
    "\n",
    "### Standard deviation <a id=\"Standarddeviation\"></a>\n",
    "<https://en.wikipedia.org/wiki/Standard_deviation>\n",
    "\n",
    "### Mean <a id=\"Mean\"></a>\n",
    "<https://en.wikipedia.org/wiki/Mean>\n",
    "\n",
    "### Variance <a id=\"Variance\"></a>\n",
    "<https://en.wikipedia.org/wiki/Variance#Matrix_notation_for_the_variance_of_a_linear_combination>\n",
    "\n",
    "### Explained variation <a id=\"Explainedvariation\"></a>\n",
    "<https://en.wikipedia.org/wiki/Explained_variation>\n",
    "\n",
    "### Explained variance in PCA <a id=\"ExplainedvarianceinPCA\"></a>\n",
    "<https://ro-che.info/articles/2017-12-11-pca-explained-variance>\n",
    "\n",
    "### Statistical dispersion <a id=\"Statisticaldispersion\"></a>\n",
    "<https://en.wikipedia.org/wiki/Statistical_dispersion>\n",
    "\n",
    "---\n",
    "\n",
    "### How are the Eigen vectors and the projection matrix plane related? <a id=\"eigenandprojection\"></a>\n",
    "<https://www.quora.com/How-are-the-Eigen-vectors-and-the-projection-matrix-plane-related#:~:text=If%20is%20a%20projection%20matrix,eigenvector%20corresponding%20to%20the%20eigenvalue%20.>\n",
    "\n",
    "\n",
    "### Eigenvalues and Eigenspaces of a Projection <a id=\"eigenandprojection2\"></a>\n",
    "<https://math.stackexchange.com/questions/1208535/eigenvalues-and-eigenspaces-of-a-projection>\n",
    "\n",
    "---\n",
    "\n",
    "### A maths dictionary for kids - Spread <a id=\"AmathsdictionaryforkidsSpread\"></a>\n",
    "<http://www.amathsdictionaryforkids.com/qr/s/spread.html>\n",
    "\n",
    "---\n",
    "\n",
    "### Learn How to Write Markdown & LaTeX in The Jupyter Notebook <a id=\"\"></a>\n",
    "<https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd>\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
