{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import math\n",
    "import cmath\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as p\n",
    "import matplotlib.pyplot as plt\n",
    "from turtle import *\n",
    "import re\n",
    "from sympy.ntheory import discrete_log\n",
    "from matplotlib.transforms import Affine2D\n",
    "import skimage.io\n",
    "import time\n",
    "# imports for egienvalues and eigenvectors\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic:\n",
    "\n",
    "Principal Component Analysis\n",
    "\n",
    "Sometimes a projection of a higher-dimensional to a lower-dimensional space is useful. It's extremely useful if we want to get some visual understanding of a, say, 15D space, in 3D or even 2D. One algorithm which allows us to project multidimensional data into fewer dimensions while keeping the most important shapes and structures is called principal component analysis (PCA). You can explore this using the following checklist:\n",
    "\n",
    "- [x] What are eigenvalues and eigenvectors?\n",
    "- [x] What is the eigenbasis? \n",
    "- [x] What is the spectrum of a matrix?\n",
    "- [x] How do we compute the eigenvalues and eigenvectors of a matrix?\n",
    "- [ ] What is projection?\n",
    "- [ ] How does projection conserve some shapes? Think about an object casting a shadow\n",
    "- [ ] How is the projection problem related to eigenvalues and eigenvectors?\n",
    "- [x] What is PCA?\n",
    "- [ ] What are principal components? How many components are there (as a function of dimensions of the original space)?\n",
    "- [ ] What is variance? What is explained variance?\n",
    "- [ ] How do principal components relate to explained variance?\n",
    "- [ ] How is PCA implemented? Implement and show.\n",
    "- [ ] Show some applications of PCA, e.g. reducing a 3D image to its first 2 principal components, plotting the 3D and 2D images.\n",
    "- [ ] Show a practical use of PCA, for example, trying to see features in a 15D space, projected in 3D.\n",
    "\n",
    "---\n",
    "\n",
    "- [ ] SVG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Motivation\n",
    "\n",
    "Here is the perspective: we are an experimenter. We are trying to understand some phenomenon by measuring various quantities (e.g. spectra, voltages, velocities, etc.) in our system. Unfortunately, we can not figure out what is happening because the data appears clouded, unclear and even redundant. This is not a trivial problem, but rather a fundamental obstacle in empirical science. Examples abound from complex systems such as neuroscience, photometry, meteorology and oceanography - the number of variables to measure can be unwieldy and at times even deceptive, because the underlying relationships can often be quite simple. Take for example a simple toy problem from physics diagrammed in Figure 1. Pretend we are studying the motion of the physicist’s ideal spring. This system consists of a ball of mass m attached to a massless, friction- less spring. The ball is released a small distance away from equilibrium (i.e. the spring is stretched). Because the spring is “ideal,” it oscillates indefinitely along the x-axis about its equilibrium at a set frequency. This is a standard problem in physics in which the motion along the x direction is solved by an explicit function of time. In other words, the underlying dynamics can be expressed as a function of a single variable x. However, being ignorant experimenters we do not know any of this. We do not know which, let alone how many, axes and dimensions are important to measure. Thus, we decide to measure the ball’s position in a three-dimensional space (since we live in a three dimensional world). Specifically, we place three movie cameras around our system of interest. At 200 Hz each movie camera records an image indicating a two dimensional position of the ball (a projection). Unfortunately, because of our ignorance, we do not even know what are the real “x”, “y” and “z” axes, so we choose three camera axes {~a, ~b,~c} at some arbitrary angles with respect to the system. The angles between our measurements might not even be 90o! Now, we record with the cameras for several minutes. The big question remains: how do we get from this data set to a simple equation of x? We know a-priori that if we were smart experimenters, we would have just measured the position along the xaxis with one camera. But this is not what happens in the real world. We often do not know which measurements best reflect the dynamics of our system in question. Furthermore, we sometimes record more dimensions than we actually need! Also, we have to deal with that pesky, real-world problem of noise. In the toy example this means that we need to deal with air, imperfect cameras or even friction in a less-than-ideal spring. Noise contaminates our data set only serving to obfuscate the dynamics further. This toy example is the challenge experimenters face everyday. [[2]](#2)[[3]](#3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables while retaining as much as possible of the variation present in the data set. This is achieved by transforming to a new set of variables, the principal components (PCs), which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.\n",
    "\n",
    "For example lets say that we have 15 dimensions but we can only view a 3 dimensional (3D) space. PCA solves this for us by reducing the dimensions from 15 to 3. Another example is reducing a 3D image to its first 2 principal components and plotting the 3D as a 2D image.\n",
    "\n",
    "So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But first...\n",
    "\n",
    "_Let's cover a few terms and definitions, so we have them at hand for clarity sake._\n",
    "\n",
    "_Please see the [Definitions](#Definitions) section._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are eigenvalues and eigenvectors?\n",
    "\n",
    "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. \n",
    "\n",
    "The corresponding eigenvalue, often denoted by λ is the factor by which the eigenvector is scaled.\n",
    "\n",
    "Let’s consider for an example that we want to build mathematical models (equations) where the input data is gathered from a large number of sources.\n",
    "\n",
    "It introduces its own sets of problems such as the large sparse matrix can end up taking a significant amount of space on a disk. Plus, it becomes extremely time-consuming for the model to train itself on the data. Furthermore, it is difficult to understand and visualize data with more than 3 dimensions, let alone a dataset of over 100+ dimensions. Hence, it would be ideal to somehow compress/transform this data into a smaller dataset.\n",
    "\n",
    "There is a solution. We can utilise Eigenvalues and Eigenvectors to reduce the dimension space. To elaborate, one of the key methodologies to improve efficiency in computationally intensive tasks is to reduce the dimensions after ensuring most of the key information is maintained.\n",
    "\n",
    "**Eigenvalues and Eigenvectors are the key tools to use in those scenarios.**\n",
    "\n",
    "**1. What Is An Eigenvector?**\n",
    "\n",
    "For the sake of simplicity, let’s consider that we live in a two-dimensional world.\n",
    "\n",
    "- Alex’s house is located at coordinates [10,10] (x=10 and y =10). Let’s refer to it as vector A.\n",
    "\n",
    "- Furthermore, his friend Bob lives in a house with coordinates [20,20] (x=20 and y=20). I will refer to it as vector B.\n",
    "\n",
    "If Alex wants to meet Bob at his place then Alex would have to travel +10 points on the x-axis and +10 points on the y-axis. This movement and direction can be represented as a two-dimensional vector [10,10]. Let’s refer to it as vector C.\n",
    "\n",
    "We can see that vector A to B are related because vector B can be achieved by scaling (multiplying) the vector A by 2. This is because 2 x [10,10] = [20,20]. This is the address of Bob. Vector C also represents the movement for A to reach B.\n",
    "\n",
    "**The key to note is that a vector can contain the magnitude and direction of a movement.**\n",
    "\n",
    "We learned from the introduction above that large set of data can be represented as a matrix and we need to somehow compress the columns of the sparse matrix to speed up our calculations. Plus if we multiply a matrix by a vector then we achieve a new vector. The multiplication of a matrix by a vector is known as transformation matrices.\n",
    "\n",
    "**We can transform and change matrices into new vectors by multiplying a matrix with a vector. The multiplication of the matrix by a vector computes a new vector. This is the transformed vector.**\n",
    "    \n",
    "- Sometimes, the new transformed vector is just a scaled form of the original vector. This means that the new vector can be re-calculated by simply multiplying a scalar (number) to the original vector; just as in the example of vector A and B above.\n",
    "    \n",
    "- And other times, the transformed vector has no direct scalar relationship with the original vector which we used to multiply to the matrix.\n",
    "\n",
    "**If the new transformed vector is just a scaled form of the original vector then the original vector is known to be an eigenvector of the original matrix. Vectors that have this characteristic are special vectors and they are known as eigenvectors. Eigenvectors can be used to represent a large dimensional matrix.**\n",
    "\n",
    "Therefore, if our input is a large sparse matrix M then we can find a vector o that can replace the matrix M. The criteria is that the product of matrix M and vector o should be the product of vector o and a scalar n:\n",
    "\n",
    "$$M * o = n* o$$\n",
    "\n",
    "This means that a matrix M and a vector o can be replaced by a scalar n and a vector o.\n",
    "In this instance, o is the eigenvector and n is the eigenvalue and our target is to find o and n.\n",
    "\n",
    "Therefore an eigenvector is a vector that does not change when a transformation is applied to it, except that it becomes a scaled version of the original vector.\n",
    "\n",
    "Eigenvectors can help us calculating an approximation of a large matrix as a smaller vector. There are many other uses which I will explain later on in the article.\n",
    "\n",
    "Eigenvectors are used to make linear transformation understandable. Think of eigenvectors as stretching/compressing an X-Y line chart without changing their direction.\n",
    "\n",
    "**2. What is an Eigenvalue?**\n",
    "\n",
    "*Eigenvalue— The scalar that is used to transform (stretch) an Eigenvector.*\n",
    "\n",
    "**3. Where are Eigenvectors and Eigenvalues used?** \n",
    "\n",
    "There are multiple uses of eigenvalues and eigenvectors:\n",
    "\n",
    "1. Eigenvalues and Eigenvectors have their importance in linear differential equations where you want to find a rate of change or when you want to maintain relationships between two variables.\n",
    "\n",
    "*Think of eigenvalues and eigenvectors as providing summary of a large matrix*\n",
    "\n",
    "2. We can represent a large set of information in a matrix. Performing computations on a large matrix is a very slow process. To elaborate, one of the key methodologies to improve efficiency in computationally intensive tasks is to reduce the dimensions after ensuring most of the key information is maintained. Hence, one eigenvalue and eigenvector are used to capture key information that is stored in a large matrix. This technique can also be used to improve the performance of data churning components.\n",
    "\n",
    "3. Component analysis is one of the key strategies that is utilised to reduce dimension space without losing valuable information. The core of component analysis (PCA) is built on the concept of eigenvalues and eigenvectors. The concept revolves around computing eigenvectors and eigenvalues of the covariance matrix of the features.\n",
    "\n",
    "4. Additionally, eigenvectors and eigenvalues are used in facial recognition techniques such as EigenFaces.\n",
    "\n",
    "5. They are used to reduce dimension space. The technique of Eigenvectors and Eigenvalues are used to compress the data. As mentioned above, many algorithms such as PCA rely on eigenvalues and eigenvectors to reduce the dimensions.\n",
    "\n",
    "*Eigenvectors and eigenvalues are used to reduce noise in data. They can help us improve efficiency in computationally intensive tasks. They also eliminate features that have a strong correlation between them.*\n",
    "\n",
    "6. Occasionally we gather data that contains a large amount of noise. Finding important or meaningful patterns within the data can be extremely difficult. Eigenvectors and eigenvalues can be used to construct spectral clustering. They are also used in singular value decomposition.\n",
    "\n",
    "7. We can also use eigenvector to rank items in a dataset. They are heavily used in search engines and calculus.\n",
    "\n",
    "8. Lastly, in non-linear motion dynamics, eigenvalues and eigenvectors can be used to help us understand the data better as they can be used to transform and represent data into manageable sets.\n",
    "\n",
    "*It can be slow to compute eigenvectors and eigenvalues. The computation is $O(n^3)$*\n",
    "\n",
    "**4. What are the building blocks of Eigenvalues and Eigenvectors?**\n",
    "\n",
    "Let’s understand the foundations of Eigenvalues and Eigenvectors.\n",
    "\n",
    "Eigenvectors and eigenvalues revolve around the concept of matrices.\n",
    "\n",
    "Matrices are used in machine learning problems to represent a large set of information. Eigenvalues and eigenvectors are all about constructing one vector with one value to represent a large matrix. Sounds very useful, right?\n",
    "\n",
    "**What is a matrix?** \n",
    "\n",
    "- A matrix has a size of X rows and Y columns, like a table.\n",
    "\n",
    "- A square matrix is the one that has a size of n, implying that X and Y are equal.\n",
    "\n",
    "- A square matrix is represented as A. This is an example of a square matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & B & C \\\\\n",
    "D & E & F \\\\\n",
    "G & H & I\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Note: Matrix properties are out of the scope of this article.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we compute the eigenvalues and eigenvectors of a matrix?\n",
    "\n",
    "Calculating Eigenvectors and Eigenvalues\n",
    "\n",
    "*Although we don’t have to calculate the Eigenvalues and Eigenvectors by hand every time but it is important to understand the inner workings to be able to confidently use the algorithms.*\n",
    "\n",
    "- The eigenvector is an array with n entries where n is the number of rows (or columns) of a square matrix. The eigenvector is represented as x.\n",
    "\n",
    "- Key Note: The direction of an eigenvector does not change when a linear transformation is applied to it.\n",
    "\n",
    "- Therefore, Eigenvector should be a non-null vector\n",
    "\n",
    "- We are required to find a number of values, known as eigenvalues such that\n",
    "\n",
    "$$ A * x = \\lambda * x$$\n",
    "\n",
    "The above equation states that we need to find eigenvalue (lambda) and eigenvector (x) such that when we multiply a scalar lambda (eigenvalue) to the vector x (eigenvector) then it should equal to the linear transformation of the matrix A once it is scaled by vector x (eigenvector).\n",
    "\n",
    "**Eigenvalues are represented as lambda.**\n",
    "\n",
    "**Note: The above equation should not be invertible.**\n",
    "\n",
    "There are two special keywords which we need to understand: Determinant of a matrix and an identity matrix.\n",
    "\n",
    "The **determinant of a matrix** is a number that is computed from a square matrix. In a nutshell, the diagonal elements are multiplied by each other and then they are subtracted together. We need to ensure that the determinant of the matrix is 0.\n",
    "\n",
    "We need an **Identity Matrix**. An identity square matrix is a matrix that has 1 in diagonal and all of its elements are 0. The identity matrix is represented as I:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We can represent\n",
    "\n",
    "$$A * x = \\lambda * x$$\n",
    "\n",
    "As:\n",
    "\n",
    "$$A * x - \\lambda * x = 0$$\n",
    "\n",
    "Now, we need to compute a characteristic equation.\n",
    "\n",
    "$$|A - \\lambda * I| = 0$$\n",
    "\n",
    "\n",
    "Subsequently:\n",
    "\n",
    "$$Determinant(A - \\lambda * I) = 0$$\n",
    "\n",
    "### How do I calculate Eigenvalue?\n",
    "\n",
    "The task is to find Eigenvalues of size n for a matrix A of size n.\n",
    "\n",
    "Therefore, the aim is to find Eigenvector and Eigenvalues of A such that:\n",
    "\n",
    "$$ A * Eigenvector — Eigenvalue * EigenVector = 0 $$\n",
    "\n",
    "**Find Lambda Such that:**\n",
    "\n",
    "$$ Determinant(A — \\lambda * I) = 0$$\n",
    "\n",
    "Based on the concepts from above:\n",
    "\n",
    "$\\lambda * I$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 0 & 0 \\\\\n",
    "0 & \\lambda & 0 \\\\\n",
    "0 & 0 & \\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "if A is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & B & C \\\\\n",
    "D & E & F \\\\\n",
    "G & H & I\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then $A - \\lambda * I$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & B & C \\\\\n",
    "D & E & F \\\\\n",
    "G & H & I\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 0 & 0 \\\\\n",
    "0 & \\lambda & 0 \\\\\n",
    "0 & 0 & \\lambda\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "A-\\lambda & B & C \\\\\n",
    "D & E-\\lambda & F \\\\\n",
    "G & H & I-\\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally calculate the determinant of $(A-\\lambda*I)$ as:\n",
    "\n",
    "$$\n",
    "A-\\lambda\n",
    "\\begin{bmatrix}\n",
    "E-\\lambda & F \\\\\n",
    "H & I-\\lambda\n",
    "\\end{bmatrix}\n",
    "-B\n",
    "\\begin{bmatrix}\n",
    "D & F \\\\\n",
    "G & I-\\lambda\n",
    "\\end{bmatrix}\n",
    "+C\n",
    "\\begin{bmatrix}\n",
    "D & E-\\lambda \\\\\n",
    "G & H\n",
    "\\end{bmatrix}\n",
    "=0\n",
    "$$\n",
    "\n",
    "Once the equation above is solved, we will get the values of lambda $\\lambda$. \n",
    "*These values are the Eigenvalues.*\n",
    "\n",
    "Once we have calculated eigenvalues, we can calculate the Eigenvectors of matrix A by using Gaussian Elimination.\n",
    "\n",
    "Gaussian elimination is about converting the matrix to row echelon form. Finally, it is about solving the linear system by back substitution.\n",
    "\n",
    "An explanation of Gaussian elimination is out of the scope of this article so that we can concentrate on Eigenvalues and Eigenvectors.\n",
    "\n",
    "**Once we have the Eigenvalues, we can find Eigenvector for each of the Eigenvalues. We can substitute the eigenvalue in the lambda and we will achieve an eigenvector.**\n",
    "\n",
    "$$(A - \\lambda * I) * x = 0$$\n",
    "\n",
    "*Therefore if a square matrix has a size n then we will get n eigenvalues and as a result, n eigenvectors will be computed to represent the matrix.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A practical example: Calculating Eigenvalue and Eigenvector\n",
    "\n",
    "**Let’s find eigenvalue of the following matrix A:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "First, multiply lambda to an identity matrix and then subtract the two matrices:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "-\\lambda\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We need to compute a determinant of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2-\\lambda & -1 \\\\\n",
    "4 & 3-\\lambda\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "det\n",
    "\\begin{bmatrix}\n",
    "2-\\lambda & -1 \\\\\n",
    "4 & 3-\\lambda\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\lambda^2 - 5\\lambda + 10\n",
    "$$\n",
    "\n",
    "Once we solve the quadratic equation above, we will yield two Eigenvalues:\n",
    "\n",
    "$$\n",
    "\\frac{5}{2} + i\\frac{\\sqrt{15}}{2},\n",
    "\\frac{5}{2} - i\\frac{\\sqrt{15}}{2}\n",
    "$$\n",
    "\n",
    "**Now that we have computed Eigenvalues, let’s calculate Eigenvectors:**\n",
    "\n",
    "Take the first Eigenvalue (Lambda $\\lambda$) and substitute the eigenvalue into the following equation:\n",
    "\n",
    "$$(A - \\lambda * I)$$\n",
    "\n",
    "We plug in the first Eigenvalue (Lambda $\\lambda$) to get the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\left(\\frac{5}{2} + i\\frac{\\sqrt{15}}{2}\\right)\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-\\frac{1}{2}-i\\frac{\\sqrt{15}}{2} & -1 \\\\\n",
    "4 & \\frac{1}{2}-i\\frac{\\sqrt{15}}{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This gives us the following Eigenvector, which is for the first eigenvalue:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 + \\sqrt{15}i \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This Eigenvector now represents the key information of matrix A.\n",
    "\n",
    "If we do the same for the other Eigenvalue we will get:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 - \\sqrt{15}i \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Eigenvalues and Eigenvectors in Python\n",
    "\n",
    "Although we don’t have to calculate the Eigenvalues and Eigenvectors by hand but it is important to understand the inner workings to be able to confidently use the algorithms. Furthermore, It is very straightforward to calculate eigenvalues and eigenvectors in Python.\n",
    "\n",
    "We can use `numpy.linalg.eig` module. It takes in a square matrix as the input and returns eigenvalues and eigenvectors. It also raises an `LinAlgError` if the eigenvalue computation does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[2.5+1.93649167j 2.5-1.93649167j]\n",
      "\n",
      "\n",
      "Eigenvectors:\n",
      "[[-0.1118034 +0.4330127j -0.1118034 -0.4330127j]\n",
      " [ 0.89442719+0.j         0.89442719-0.j       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "input = np.array([[2,-1],[4,3]])\n",
    "w, v = LA.eig(input)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(w)\n",
    "print(\"\\n\")\n",
    "print(\"Eigenvectors:\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the eigenbasis? \n",
    "\n",
    "Eigenbasis is a basis for a vector space consisting entirely of eigenvectors.\n",
    "\n",
    "### Eigenbasis and Diagonalization\n",
    "\n",
    "Given that we know that a transformation can have up to $n$ Eigenvectors, where $n$ is the number of rows, what happens if we use the Eigenvectors as a **change of basis**, by multiplying the transformation by the matrix of the Eigenvectors?\n",
    "\n",
    "As it turns out, converting the transformation to an Eigenbasis, if possible, (a conversion otherwise known as **Eigendecomposition**) is an incredibly useful conversion because of what happens to the transformation when it is converted in such a way.\n",
    "\n",
    "Take for example, the matrix \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "\n",
    "This matrix scales by a factor of 2 along the y-axis, shears along the xz-axis by a factor of 1.\n",
    "\n",
    "This transformation has Eigenvalues $\\lambda = 2$ and $\\lambda = 1$ with algebraic multiplicity 2.\n",
    "\n",
    "It also has Eigenvectors \n",
    "$\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}$\n",
    ", for \n",
    "$\\lambda = 2$, and $\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-1 \\\\\n",
    "1\n",
    "\\end{bmatrix}$\n",
    "and\n",
    "$\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}$\n",
    ", for $\\lambda = 1$.\n",
    "\n",
    "These Eigenvectors can be arranged into a new matrix called an **Eigenbasis**:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the inverse of the Eigenbasis can be found too:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0& 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's consider what happens if we change the basis of our matrix by premultiplying by the inverse of the Eigenbasis, then postmultiplying by the Eigenbasis (a transformation also known as an **Eigendecomposition**):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0& 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here it is important to note the following -  the result of changing the basis to a matrix to its Eigenbasis is that the matrix is put into a **Diagonalized form**. This is extremely useful, because while the matrix is in a diagonalized form, we can represent it like this:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\\cdot I\n",
    "$$\n",
    "\n",
    "Thus, if we want to apply any matrix multiplication operation to the matrix in its diagonalized form, it is the same as applying a matrix-vector optimization. Computer Scientists will recognize this as a huge performance win, since an \n",
    "$O(N^2)$ operation just became $O(N)$. Say for example we wanted to calcalculate the 16th power of the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Conventionally, this would take $9^2 \\times 16 = 1296$ operations. If we did the same thing on the diagonal, we can exploit the fact that we are exponentiating by powers of two and same thing would take just three barrel-shift operations, preceded by and followed by a normal matrix multiplication to undo the diagonalization.\n",
    "\n",
    "Given a set of eigenvectors and eigenvalues for a matrix, we can re-construct the original matrix. Why is this the case? Notice that when we decomposed the matrix, we did the following:\n",
    "\n",
    "$$\n",
    "AP = PD \\\\\n",
    "P^{-1}AP = D\n",
    "$$\n",
    "\n",
    "Where $P$ was our matrix of eigenvectors, $A$ was our original matrix that underwent eigendecomposition and $D$ is the eigendecomposed matrix.\n",
    "\n",
    "A property of eigenvalues is that multiplying the original matrix $A$ by an eigenvector $V$ is the same as multiplying that eigenvector by its eigenvalue $\\lambda$. All the multiplication does in both cases is scale the vector.\n",
    "\n",
    "This is the same thing if you multiply by a matrix that only has elements on the diagonal - the effect is scaling, regardless of whether the multiplication was a premultiplication or a postmultiplication. So it stands to reason that if you were to arrange the eigenvalues into a diagonal matrix $E$ with their columns corresponding to each eigenvector in the matrix $P$, then the following, just like above with $AP = PD$, holds true:\n",
    "\n",
    "$$AP = PE$$\n",
    "\n",
    "Lets see if this checks out:\n",
    "\n",
    "$$\n",
    "AP = \n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 2 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE = \n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 2 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Same thing! So now we can do the same thing as before - postmultiply both sides by $P^{-1}$ and it should be the case that we recover $A$, eg:\n",
    "\n",
    "$$\n",
    "APP^{-1} = PEP^{-1} \\\\\n",
    "A = PEP^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0& 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1& 0 & 0 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the spectrum of a matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now that we have covered some ground, before we proceed, let's step back and cover some Fundamental theory of matrix eigenvectors and eigenvalues._\n",
    "\n",
    "### Fundamental theory of matrix eigenvectors and eigenvalues \n",
    "[[1]](#1)\n",
    "\n",
    "A (non-zero) vector $v$ of dimension $N$ is an eigenvector of a square $N \\times N$ matrix $A$ if it satisfies the linear equation:\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "where $\\lambda$ is a scalar, termed the **eigenvalue** corresponding to $v$. That is, the eigenvectors are the vectors that the linear transformation $A$ merely elongates or shrinks, and the amount that they elongate/shrink by is the **eigenvalue**. The above equation is called the **eigenvalue equation** or the **eigenvalue problem**.\n",
    "\n",
    "This yields an equation for the eigenvalues:\n",
    "\n",
    "$$ p\\left(\\lambda \\right)=\\det \\left(\\mathbf{A} -\\lambda \\mathbf{I} \\right)=0$$\n",
    "\n",
    "We call $p(\\lambda)$ the characteristic polynomial, and the equation, called the characteristic equation, is an $Nth$ order polynomial equation in the unknown $\\lambda$. This equation will have $N\\lambda$ distinct solutions, where $1 \\leq N\\lambda \\leq N$. The set of solutions, that is, the eigenvalues, is called the **spectrum** of $A$.\n",
    "\n",
    "We can factor $p$ as:\n",
    "\n",
    "$$p\\left(\\lambda \\right)=\\left(\\lambda -\\lambda_{1}\\right)^{n_{1}}\\left(\\lambda -\\lambda _{2}\\right)^{n_{2}}\\cdots \\left(\\lambda -\\lambda _{N_{\\lambda }}\\right)^{n_{N_{\\lambda }}}=0$$\n",
    "\n",
    "The integer $n_i$ is termed the algebraic multiplicity of eigenvalue $\\lambda_i$. If the field of scalars is algebraically closed, the algebraic multiplicities sum to $N$:\n",
    "\n",
    "$${\\sum \\limits _{i=1}^{N_{\\lambda }}{n_{i}}=N}$$\n",
    "\n",
    "For each eigenvalue $\\lambda_i$, we have a specific eigenvalue equation:\n",
    "\n",
    "$${\\displaystyle \\left(\\mathbf {A} -\\lambda _{i}\\mathbf {I} \\right)\\mathbf {v} =0}$$\n",
    "\n",
    "There will be $1 \\leq m_i \\leq n_i$ linearly independent solutions to each eigenvalue equation. The linear combinations of the $m_i$ solutions are the eigenvectors associated with the eigenvalue $\\lambda_i$. The integer mi is termed the geometric multiplicity of $$\\lambda_i$$. It is important to keep in mind that the algebraic multiplicity $n_i$ and geometric multiplicity $m_i$ may or may not be equal, but we always have $m_i \\leq n_i$. The simplest case is of course when $m_i = n_i = 1$. The total number of linearly independent eigenvectors, $N_v$, can be calculated by summing the geometric multiplicities:\n",
    "\n",
    "$${\\displaystyle \\sum \\limits _{i=1}^{N_{\\lambda }}{m_{i}}=N_{\\mathbf {v} }}$$\n",
    "\n",
    "The eigenvectors can be indexed by eigenvalues, using a double index, with $v_{ij}$ being the $jth$ eigenvector for the $ith$ eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index $v_k$, with $k = 1, 2, ..., N_v$.\n",
    "\n",
    "_Note: algebraic multiplicity and geometric multiplicity are out of the scope of this article._\n",
    "\n",
    "### Spectrum of a matrix \n",
    "[[Spectrum of a matrix]](#Spectrumofamatrix)\n",
    "\n",
    "In mathematics, the spectrum of a matrix is the set of its eigenvalues. More generally, if ${\\displaystyle T\\colon V\\to V}$ is a linear operator over any finite-dimensional vector space, its spectrum is the set of scalars ${\\displaystyle \\lambda }$  such that ${\\displaystyle T-\\lambda I}$ I is not **invertible**. The determinant of the matrix equals the product of its eigenvalues. Similarly, the **trace** of the matrix equals the sum of its eigenvalues. From this point of view, we can define the **pseudo-determinant** for a **singular matrix** to be the product of its nonzero eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does projection conserve some shapes? Think about an object casting a shadow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the projection problem related to eigenvalues and eigenvectors?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are principal components? How many components are there (as a function of dimensions of the original space)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is variance? What is explained variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do principal components relate to explained variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is PCA implemented? Implement and show.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some applications of PCA, e.g. reducing a 3D image to its first 2 principal components, plotting the 3D and 2D images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a practical use of PCA, for example, trying to see features in a 15D space, projected in 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "<a id=\"Definitions\"></a>\n",
    "\n",
    "### Euclidean vector\n",
    "\n",
    "In mathematics, physics and engineering, a Euclidean vector or simply a vector (sometimes called a geometric vector or spatial vector) is a geometric object that has magnitude (or length) and direction. Vectors can be added to other vectors according to vector algebra. A Euclidean vector is frequently represented by a ray (a line segment with a definite direction), or graphically as an arrow connecting an initial point $A$ with a terminal point $B$, and denoted by $\\overrightarrow{AB}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.annotate('A', xy=(-0.3, -0.3))\n",
    "ax.annotate('B', xy=(2.1, 3.1))\n",
    "ax.text(0.5, 1.5, '$\\overrightarrow{a}$')\n",
    "\n",
    "plt.quiver(0, 0, 2, 3, scale_units = \"xy\", angles = \"xy\", scale = 1, color ='b')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "# DO NOT USE plt.axis(\"equal\") - will not work, or worse will not ALWAYS work, use plt.gca().set_aspect(\"equal\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant of a square matrix\n",
    "[[Determinant]](#Determinant)\n",
    "\n",
    "In mathematics, the determinant is a scalar value that is a function of the entries of a square matrix. It allows characterizing some properties of the matrix and the linear map represented by the matrix. In particular, the determinant is nonzero if and only if the matrix is **invertible**, and the linear map represented by the matrix is an isomorphism. The determinant of a product of matrices is the product of their determinants (the preceding property is a corollary of this one). The determinant of a matrix A is denoted $det(A)$, $det A$, or $|A|$.\n",
    "\n",
    "In the case of a $2 \\times 2$ matrix the determinant can be defined as:\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}|A|={\\begin{vmatrix}a&b\\\\c&d\\end{vmatrix}}=ad-bc.\\end{aligned}}}$$\n",
    "\n",
    "Similarly, for a 3 × 3 matrix A, its determinant is\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}|A|={\\begin{vmatrix}a&b&c\\\\d&e&f\\\\g&h&i\\end{vmatrix}}&=a\\,{\\begin{vmatrix}e&f\\\\h&i\\end{vmatrix}}-b\\,{\\begin{vmatrix}d&f\\\\g&i\\end{vmatrix}}+c\\,{\\begin{vmatrix}d&e\\\\g&h\\end{vmatrix}}\\\\[3pt]&=aei+bfg+cdh-ceg-bdi-afh.\\end{aligned}}}$$\n",
    "\n",
    "Each determinant of a 2 × 2 matrix in this equation is called a minor of the matrix A. \n",
    "\n",
    "#### Definition\n",
    "\n",
    "There are various equivalent ways to define the determinant of a square matrix $A$, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective **minors**; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a $4 × 4$ matrix:\n",
    "\n",
    "$${\\displaystyle {\\begin{vmatrix}a&b&c&d\\\\e&f&g&h\\\\i&j&k&l\\\\m&n&o&p\\end{vmatrix}}=a\\,{\\begin{vmatrix}f&g&h\\\\j&k&l\\\\n&o&p\\end{vmatrix}}-b\\,{\\begin{vmatrix}e&g&h\\\\i&k&l\\\\m&o&p\\end{vmatrix}}+c\\,{\\begin{vmatrix}e&f&h\\\\i&j&l\\\\m&n&p\\end{vmatrix}}-d\\,{\\begin{vmatrix}e&f&g\\\\i&j&k\\\\m&n&o\\end{vmatrix}}}$$\n",
    "\n",
    "### Minor\n",
    "[[Minor]](#Minor)\n",
    "\n",
    "In linear algebra, a minor of a matrix $A$ is the determinant of some smaller square matrix, cut down from A by removing one or more of its rows and columns.\n",
    "\n",
    "### Linear isomorphism\n",
    "[[Linear map]](#Linearmap)\n",
    "\n",
    "In mathematics, a linear map (also called a linear mapping, linear transformation, vector space homomorphism, or in some contexts linear function) is a mapping ${\\displaystyle V\\rightarrow W}$ between two vector spaces that preserves the operations of vector addition and scalar multiplication.\n",
    "If a linear map is a bijection then it is called a **linear isomorphism**.\n",
    "\n",
    "### Bijection\n",
    "[[Bijection]](#Bijection)\n",
    "\n",
    "In mathematics, a bijection, bijective function, one-to-one correspondence, or invertible function, is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. There are no unpaired elements. \n",
    "\n",
    "### Pseudo-determinant\n",
    "[[Pseudo-determinant]](#Pseudodeterminant)\n",
    "\n",
    "In linear algebra and statistics, the **pseudo-determinant** is the product of all non-zero eigenvalues of a square matrix. It coincides with the regular determinant when the matrix is **non-singular**.\n",
    "\n",
    "### Invertible matrix\n",
    "[[Invertible matrix]](#Invertiblematrix)\n",
    "\n",
    "In linear algebra, an $n-by-n$ square matrix $A$ is called invertible (also **nonsingular** or **nondegenerate**), if there exists an $n-by-n$ square matrix B such that:\n",
    "\n",
    "$${\\displaystyle \\mathbf {AB} =\\mathbf {BA} =\\mathbf {I} _{n}\\ }$$\n",
    "\n",
    "where $I_n$ denotes the $n-by-n$ identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix $B$ is uniquely determined by $A$, and is called the (multiplicative) **inverse** of $A$, denoted by $A^{−1}$. Matrix inversion is the process of finding the matrix $B$ that satisfies the prior equation for a given **invertible** matrix $A$.\n",
    "\n",
    "A square matrix that is **not invertible** is called **singular** or **degenerate**. A square matrix is **singular** _if and only if_ its determinant is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "- PCA - Principal Component Analysis\n",
    "- 3D - Three Dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "### PCA\n",
    "\n",
    "<https://www.cs.cmu.edu/~tom/10701_sp11/slides/pca_schlens.pdf> <a id=2>[2]</a>\n",
    "\n",
    "<https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf> <a id=2>[3]</a>\n",
    "\n",
    "<https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>\n",
    "\n",
    "<https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Principal_component_analysis>\n",
    "\n",
    "<https://builtin.com/data-science/step-step-explanation-principal-component-analysis>\n",
    "\n",
    "<https://heartbeat.fritz.ai/understanding-the-mathematics-behind-principal-component-analysis-efd7c9ff0bb3>\n",
    "\n",
    "<https://medium.com/analytics-vidhya/mathematics-behind-principal-component-analysis-pca-1cdff0a808a9>\n",
    "\n",
    "<https://www.nature.com/articles/nmeth.4346>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Norm_(mathematics)>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Euclidean_distance>\n",
    "\n",
    "---\n",
    "\n",
    "### General\n",
    "\n",
    "<https://www.wolframalpha.com/>\n",
    "\n",
    "---\n",
    "\n",
    "### 3Blue1Brown\n",
    "<https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw>\n",
    "\n",
    "---\n",
    "\n",
    "### StatQuest with Josh Starmer\n",
    "<https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw/search?query=pca>\n",
    "\n",
    "---\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors>\n",
    "\n",
    "<https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices>\n",
    "\n",
    "---\n",
    "\n",
    "### Eigenbasis\n",
    "\n",
    "<https://intuitive-math.club/linear-algebra/eigenbasis/>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix> <a id=\"1\">[1]</a>\n",
    "\n",
    "<https://canvas.harvard.edu/files/3780067/download?download_frd=1&verifier=hNHLukPIpGtkoApu6WIE51qatsQV7VNvCCak7jwW>\n",
    "\n",
    "<http://www.math.lsa.umich.edu/~kesmith/EigenEverything2017.pdf>\n",
    "\n",
    "<https://math.stackexchange.com/questions/723762/eigenspace-what-is-it>\n",
    "\n",
    "<https://math.stackexchange.com/questions/36815/a-simple-explanation-of-eigenvectors-and-eigenvalues-with-big-picture-ideas-of>\n",
    "\n",
    "### Spectrum of a matrix\n",
    "<a id=\"Spectrumofamatrix\"></a>\n",
    "<https://en.wikipedia.org/wiki/Spectrum_of_a_matrix> \n",
    "\n",
    "### Determinant <a id=\"Determinant\"></a>\n",
    "<https://en.wikipedia.org/wiki/Determinant>\n",
    "\n",
    "### Minor <a id=\"Minor\"></a>\n",
    "<https://en.wikipedia.org/wiki/Minor_(linear_algebra)>\n",
    "\n",
    "### Linear map <a id=\"Linearmap\"></a>\n",
    "<https://en.wikipedia.org/wiki/Linear_map>\n",
    "\n",
    "### Bijection <a id=\"Bijection\"></a>\n",
    "<https://en.wikipedia.org/wiki/Bijection>\n",
    "\n",
    "### Pseudo-determinant <a id=\"Pseudodeterminant\"></a>\n",
    "<https://en.wikipedia.org/wiki/Pseudo-determinant>\n",
    "\n",
    "### Invertible matrix <a id=\"Invertiblematrix\"></a>\n",
    "<https://en.wikipedia.org/wiki/Invertible_matrix>\n",
    "\n",
    "### Orthogonal matrix\n",
    "<https://en.wikipedia.org/wiki/Orthogonal_matrix>\n",
    "\n",
    "### Orthonormality\n",
    "<https://en.wikipedia.org/wiki/Orthonormality>\n",
    "\n",
    "### Spectrum of a matrix\n",
    "<https://en.wikipedia.org/wiki/Spectrum_of_a_matrix>\n",
    "\n",
    "### Trace (linear algebra)\n",
    "<https://en.wikipedia.org/wiki/Trace_(linear_algebra)>\n",
    "\n",
    "---\n",
    "\n",
    "### Euclidean vector\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Euclidean_vector>\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
